{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "\n",
    "__Author__: Pat McCornack\n",
    "\n",
    "__Date__: 5/28/2024\n",
    "\n",
    "__Purpose__: Update fuelscape data to reflect most recent disturbances on the landscape. This script will both train models to predict FVT/FVC/FVH/F40 and apply those models to generated predicted rasters of each target. Alternatively, the user can specify existing models to be used to generate the predicted rasters. Inputs are 1. Paths to sample point layers to train the models, 2. paths to LANDFIRE input raster datasets. \n",
    "\n",
    "__Instructions__:\n",
    "If generating new models and applying them to predict rasters then update the file paths then hit 'Run All'. This will train new models and apply them to generate predicted rasters. Predicted FVT/FVC/FVH rasters are used as inputs to predict F40. \n",
    "\n",
    "If using existing models, then update the file paths then see step 3. below. This will still use the predicted FVT/FVC/FVH rasters as inputs to predict F40. \n",
    "\n",
    "1. Update paths if necessary: \n",
    "- Update the sample points file paths\n",
    "- Update the model file paths\n",
    "- Update the raster_fpaths_dict with most recent rasters\n",
    "\n",
    "__Main__\n",
    "2. Train models: If creating new models, then run cell under _Train Models_. By default it will create a model for each FVT/FVC/FVH/F40. Modify 'targets' if only creating a subset of these models. \n",
    "\n",
    "3. Predict rasters: If using existing models then specify in 'model_fpaths_dict' under _Run Model Predictions_ in the form 'target' : 'path' (e.g. 'LF22_FVT' : 'path_to_model'). Otherwise simply run the cell.  \n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import datetime as dt\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.windows import Window\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Paths\n",
    "Define set of filepaths to conveniently switch between working off local files or the PNNL drive. Set active_data_dir to either local_data_dir or pnnl_data_dir depending on which you're working off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_root_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\"\n",
    "pnnl_root_dir = r\"\\\\pnl\\projects\\BPAWildfire\\data\\Landfire\\fuels_modeling\\fuelscape_modeling\"\n",
    "\n",
    "## ! Specify these ##\n",
    "\n",
    "# Define which data directory to work off of\n",
    "active_root_dir = local_root_dir\n",
    "\n",
    "# Define paths to sample points\n",
    "sample_points_dir = os.path.join(active_root_dir, r\"sample_points\")\n",
    "sample_points_fnames = {\n",
    "    \"LF22_F40\" : \"LF22_F40_sample_points_2024-05-29_100k.shp\",\n",
    "    \"LF22_FVT\" : \"LF22_FVT_Disturbed_sample_points_2024-05-29_200k.shp\",\n",
    "    \"LF22_FVC\" : \"LF22_FVT_Disturbed_sample_points_2024-05-29_200k.shp\",\n",
    "    \"LF22_FVH\" : \"LF22_FVT_Disturbed_sample_points_2024-05-29_200k.shp\"\n",
    "}\n",
    "\n",
    "# Define paths to models\n",
    "models_dir = os.path.join(active_root_dir, \"models\")\n",
    "models_fpath_dict = {\n",
    "    'LF22_F40' : os.path.join(models_dir, \"LF22_F40_model_2024-05-29_09-28-44\"),\n",
    "    'LF22_FVT' : os.path.join(models_dir, \"LF22_FVT_HGBC_model_2024-05-29_09-37-32\"),\n",
    "    'LF22_FVC' : os.path.join(models_dir, \"LF22_FVC_HGBC_model_2024-05-29_09-31-57\"),\n",
    "    'LF22_FVH' : os.path.join(models_dir, \"LF22_FVH_HGBC_model_2024-05-29_09-35-38\"),\n",
    "}\n",
    "\n",
    "\n",
    "## ! These likely don't need to be updated ##\n",
    "\n",
    "# Directory paths\n",
    "paths_dict = {\n",
    "    \"out_base_dir\" : os.path.join(active_root_dir, r\"model_outputs\\tabular\"),  # Where to save result outputs \n",
    "    \"ref_data_dir\" : os.path.join(active_root_dir, r\"..\\LF_raster_data\\_tables\"),  # Location of LF csvs (e.g. LF22_FVT_230.csv)\n",
    "}\n",
    "\n",
    "# Define the source raster file paths\n",
    "data_dir =  os.path.join(active_root_dir, r'..\\LF_raster_data\\bpa_service_territory')\n",
    "ref_data_dir = os.path.join(data_dir, r\"..\\_tables\")\n",
    "raster_fpaths_dict = {\n",
    "    \"LF20_F40\" : os.path.join(data_dir, \"LC22_F40_220_bpa.tif\"),\n",
    "    \"LF20_FVT\" : os.path.join(data_dir, \"LC22_FVT_220_bpa.tif\"),\n",
    "    \"LF22_FVT\" : os.path.join(data_dir, \"LC22_FVT_230_bpa.tif\"),\n",
    "    \"LF20_FVC\" : os.path.join(data_dir, \"LC22_FVC_220_bpa.tif\"),\n",
    "    \"LF22_FVC\" : os.path.join(data_dir, \"LC22_FVC_230_bpa.tif\"),\n",
    "    \"LF20_FVH\" : os.path.join(data_dir, \"LC22_FVH_220_bpa.tif\"),\n",
    "    \"LF22_FVH\" : os.path.join(data_dir, \"LC22_FVH_230_bpa.tif\"),\n",
    "    \"LF22_FDST\" : os.path.join(data_dir, \"LC22_FDst_230_bpa.tif\"),\n",
    "    \"BPS\" : os.path.join(data_dir, \"LC20_BPS_220_bpa.tif\"),\n",
    "    \"ZONE\" : os.path.join(data_dir, \"us_lf_zones_bpa.tif\"),\n",
    "    \"ASPECT\" : os.path.join(data_dir, \"LC20_Asp_220_bpa.tif\"),\n",
    "    \"SLOPE\" : os.path.join(data_dir, \"LC20_SlpD_220_bpa.tif\"),\n",
    "    \"ELEVATION\" : os.path.join(data_dir, \"LC20_Elev_220_bpa.tif\"),\n",
    "    \"BPS_FRG_NE\" : os.path.join(data_dir, \"BPS_FRG_NEW.tif\")\n",
    "}\n",
    "\n",
    "out_chunk_dir = os.path.join(active_root_dir, r\"outputs\\geospatial\")\n",
    "out_raster_dir = os.path.join(data_dir, r'_predicted_rasters')\n",
    "out_fname_dict = {\n",
    "    'LF22_F40' : 'Pred_LF22_F40',\n",
    "    'LF22_FVT' : 'Pred_LF22_FVT',\n",
    "    'LF22_FVC' : 'Pred_LF22_FVC',\n",
    "    'LF22_FVH' : 'Pred_LF22_FVH'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Functions__\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Helper Functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output Directory\n",
    "Names the output directory using the datetime that the script was run. \n",
    "Returns the name of the directory. The returned directory is used to output the trained model and/or results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(base_dir, new_dir_name='model_results'):\n",
    "        \"\"\"\n",
    "        Returns path to a directory created at the specified base_dir location. \n",
    "\n",
    "        The name of the created directory can optionally be specified using the dir_name argument. \n",
    "        \"\"\"\n",
    "\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        output_dir = os.path.join(base_dir, new_dir_name + \"_\" + datetime)\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Dictionaries to Append Features\n",
    "Some features are separate attributes of the LANDFIRE dataset (e.g. BPS Fire Regime) and others are useful for results analysis (e.g. FDst attributes). These can be mapped to points using LANDFIRE CSVs. The below creates dictionaries to perform that mapping. \n",
    "\n",
    "This function is called by join_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref_data(ref_data_dir=paths_dict[\"ref_data_dir\"]):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of dictionaries of mappings between LANDFIRE raster values and other attributes associated with those values. \n",
    "    \"\"\"\n",
    "    data_dir = ref_data_dir\n",
    "    BPS_fname = \"LF20_BPS_220.csv\"\n",
    "   \n",
    "    # Create empty dictionary\n",
    "    LF_ref_dicts = {}\n",
    "\n",
    "    # Get BPS reference dictionary\n",
    "    BPS_df = pd.read_csv(os.path.join(data_dir, BPS_fname))\n",
    "    LF_ref_dicts[\"BPS_NAME\"] = dict(BPS_df[['VALUE', 'BPS_NAME']].values)\n",
    "    LF_ref_dicts[\"BPS_FRG_NE\"] = dict(BPS_df[['VALUE', 'FRG_NEW']].values)\n",
    "\n",
    "    return LF_ref_dicts\n",
    "                         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Features using Data Dictionaries\n",
    "Append in selected features using the LANDFIRE data dictionaries.\n",
    "\n",
    "__Note:__ Items in feature_list must be in the source_layers dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(sample_points, feature_list = ['BPS_NAME']):\n",
    "    \"\"\"\n",
    "    Returns the sample_points layer with the features in feature_list appended. \n",
    "\n",
    "    Items in feature_list must be in the source_layers dictionary.  \n",
    "    \"\"\"\n",
    "    \n",
    "    LF_ref_dicts = read_ref_data()\n",
    "    \n",
    "    source_layers = {\n",
    "        'BPS_NAME' : 'BPS',\n",
    "        'BPS_FRG_NE' : 'BPS',\n",
    "    }\n",
    "\n",
    "    # Iterate through feature_list and append features to sample_points\n",
    "    for feature in feature_list:\n",
    "        sample_points[feature] = sample_points[source_layers[feature]].map(LF_ref_dicts[feature]).copy()\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Count Filter\n",
    "Some classes have exceedingly low representation in the dataset. In order to run train_test_split, the target class must have a count greater than 1. The following will filter out classes with counts of 1. Given the size of the dataset excluding these classes will not detrimentally impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_count_filter(sample_points, target):  \n",
    "    \"\"\"\n",
    "    Returns modified sample_points dataframe. Classes in the target feature that have counts of 1 are removed. \n",
    "    \"\"\"  \n",
    "    # Group by target\n",
    "    obs_counts = sample_points.groupby(target).count()\n",
    "\n",
    "    # Identify classes with low observation counts\n",
    "    low_count_classes = obs_counts[obs_counts.iloc[:,0] < 5].index.tolist()\n",
    "\n",
    "    # Remove those classes from sample_points\n",
    "    sample_points = sample_points.loc[~sample_points[target].isin(low_count_classes)]\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Model Functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Sample Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_prep(sample_point_fpath, target):\n",
    "    \"\"\"\n",
    "    Returns dataframe of processed sample points. Sample points are read in from shapefile. \n",
    "    \"\"\"\n",
    "    # Read in gdf\n",
    "    sample_points = gpd.read_file(sample_point_fpath)\n",
    "\n",
    "    # Filter out near points\n",
    "    sample_points = sample_points.loc[sample_points['NEAR_FID'] == -1]\n",
    "\n",
    "    # Drop unneeeded columns if present\n",
    "    sample_points = sample_points.drop(['Classified', 'GrndTruth', 'NEAR_FID', 'NEAR_DIST'], axis=1,\n",
    "                                       errors='ignore')\n",
    "    \n",
    "    # Remove observations with -9999/-1111/-32768 in any field \n",
    "    sample_points = sample_points.loc[~sample_points.isin([-1111, -9999, -32768]).any(axis=1)]\n",
    "\n",
    "    # Join in additional features\n",
    "    sample_points = join_features(sample_points)\n",
    "\n",
    "    # Drop non-disturbed values if not predicting F40\n",
    "    if target != 'LF22_F40':     \n",
    "        # Filter out points that weren't disturbed\n",
    "        sample_points = sample_points.loc[sample_points['LF22_FDST'] != 0]\n",
    "\n",
    "    # Filter classes based on target\n",
    "    if target == 'LF22_F40':\n",
    "        F40_NB = [91, 93]\n",
    "        sample_points = sample_points.loc[~sample_points['LF22_F40'].isin(F40_NB)]\n",
    "\n",
    "    elif target == 'LF22_FVT':\n",
    "        developed_fvt = list(range(20,33)) + list(range(2901,2906))\n",
    "        ag_fvt = [80, 81, 82] + list(range(2960, 2971))\n",
    "        fvt_filter = developed_fvt + ag_fvt\n",
    "        sample_points = sample_points.loc[~sample_points['LF22_FVT'].isin(fvt_filter)]\n",
    "\n",
    "    elif target == 'LF22_FVH' or target == 'LF22_FVC':\n",
    "        filter = list(range(20, 70)) + list(range(80, 86))\n",
    "        sample_points = sample_points.loc[~sample_points[target].isin(filter)]\n",
    "\n",
    "    # Filter out classes in the target feature with very low counts\n",
    "    sample_points = low_count_filter(sample_points, target)\n",
    "\n",
    "    return sample_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Histogram Based Gradient Boosting Classifier\n",
    "Scikit-learn implementation of Histogram Based Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histGradientBoostingClassifier(categorical_feature_list, class_weight='balanced', seed=1234):\n",
    "    \"\"\"\n",
    "    Returns specified histogram-based gradient boosting classifier. \n",
    "    \"\"\"\n",
    "\n",
    "    hgb_classifier = HistGradientBoostingClassifier(\n",
    "        categorical_features=categorical_feature_list,  # Natively handle categorical variables\n",
    "        class_weight=class_weight,\n",
    "        random_state=seed,\n",
    "        learning_rate=0.01,\n",
    "        max_iter=100\n",
    "    )\n",
    "\n",
    "    return hgb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Trains and returns HGBC model. Subsets the training data to predictors based on the target model and predicts the specified target feature.\n",
    "\n",
    "__Note__: Aspect, Elevation, and Slope are the only continuous LANDFIRE datasets, therefore any feature not in that list is assumed to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train_data, predictors, target):\n",
    "    \"\"\"\n",
    "    Returns trained model of specified type given a target. \n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = \"balanced\"\n",
    "\n",
    "    # Get list of predictors for run\n",
    "    cat_features = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]  # Categorical features for HGBC models  \n",
    "    \n",
    "    # Separate training data predictors/response\n",
    "    y_train = train_data[target].copy()\n",
    "    X_train = train_data[predictors].copy()\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = histGradientBoostingClassifier(categorical_feature_list=cat_features, class_weight=class_weight)\n",
    "      \n",
    "    # Fit the model with the training data\n",
    "    print(f'Fitting {target} Model...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the fit model\n",
    "    return model         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model wrapper function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_wrapper(target, sample_points_fname):\n",
    "    \"\"\"\n",
    "    Trains and saves model based on specified target and returns path to saved model.\n",
    "    \"\"\"\n",
    "    # Specify predictors based on target. \n",
    "    predictors_dict = {\n",
    "        'LF22_FVT' : ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH'],\n",
    "        'LF22_FVC' : ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "        'LF22_FVH' : ['LF22_FVT', 'LF20_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "        'LF22_F40' : ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'ZONE', 'BPS_FRG_NE']\n",
    "    }\n",
    "\n",
    "    # Define path to sample points\n",
    "    sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], sample_points_fname)\n",
    "\n",
    "    # Read in and prepare data\n",
    "    sample_points = model_data_prep(sample_points_fpath, target)\n",
    "\n",
    "    # Train model\n",
    "    model = fit_model(train_data=sample_points, predictors=predictors_dict[target], target=target)\n",
    "\n",
    "    # Save model \n",
    "    datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    model_fname = f\"{target}_HGBC_model_{datetime}\"\n",
    "    model_fpath = os.path.join(paths_dict['model_dir'], model_fname)\n",
    "    dump(model, model_fpath)\n",
    "    print(f\"{target} model written to: {model_fpath}\")\n",
    "    return model_fpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Raster Predict Functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess window dataframe\n",
    "Prepares the data to be run through the model. Separates out null and non-burnable values. Returns a dictionary with: \n",
    "1. A clean dataframe to be run through the model. \n",
    "2. A dataframe of the dropped observations to be rejoined to model predictions. This allows for the data to be reshaped to a 2D numpy array and written as a raster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_prep(df, target):\n",
    "    \"\"\"\n",
    "    Prepares data to run model on.\n",
    "    Input: \n",
    "    - A dataframe containing created using the flattened numpy arrays returned from reading in the raster chunks. \n",
    "    Returns: \n",
    "    - Clean dataframe without NULL data or agricultural/developed/nonburnable classes.\n",
    "    - Dataframe with the dropped observations - to be reappended after prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Based on the target, rename the dropped LF column to Pred column\n",
    "    column_dict = {\n",
    "        \"LF22_F40\" : {'LF20_F40' : 'F40_Pred'},\n",
    "        \"LF22_FVC\" : {\"LF20_FVC\" : \"FVC_Pred\"},\n",
    "        \"LF22_FVH\" : {\"LF20_FVH\" : \"FVH_Pred\"},\n",
    "        \"LF22_FVT\" : {\"LF20_FVT\" : \"FVT_Pred\"}\n",
    "    }\n",
    "\n",
    "    pred_dict = {\n",
    "        \"LF22_F40\" : \"F40_Pred\",\n",
    "        \"LF22_FVC\" : \"FVC_Pred\",\n",
    "        \"LF22_FVH\" : \"FVH_Pred\",\n",
    "        \"LF22_FVT\" : \"FVT_Pred\"\n",
    "    }\n",
    "\n",
    "    # Remove -9999/-1111/-32768 (Null values)\n",
    "    null = df[(df.isin([-1111, -9999, -32768])).any(axis=1)] \n",
    "    df = df.drop(null.index, axis=0) \n",
    "\n",
    "    # If not predicting F40 - join in BPS_NAME as predictor and drop undisturbed points\n",
    "    # Drop non-disturbed values if not predicting F40\n",
    "    if target != 'LF22_F40':\n",
    "        df = join_features(df, feature_list = ['BPS_NAME'])\n",
    "        non_disturbed = df.loc[df['LF22_FDST'] == 0]\n",
    "        df = df.drop(non_disturbed.index, axis=0)\n",
    "\n",
    "    # Filter classes based on the target\n",
    "    if target == 'LF22_F40':\n",
    "        F40_NB = [91, 93]  # Nonburnable F40 Classes\n",
    "        filtered = df.loc[df['LF20_F40'].isin(F40_NB)]  # Drop NB classes\n",
    "        df = df.drop(filtered.index, axis=0)\n",
    "        \n",
    "    elif target == 'LF22_FVC':\n",
    "        fvc_filter = list(range(20, 70)) + list(range(80, 86)) # Agricultural/Developed classes\n",
    "        filtered = df.loc[df['LF20_FVC'].isin(fvc_filter)]\n",
    "        df = df.drop(filtered.index, axis=0)\n",
    "    \n",
    "    elif target == 'LF22_FVH':\n",
    "        fvh_filter = list(range(20, 70)) + list(range(80, 86)) # Agricultural/Developed classes\n",
    "        filtered = df.loc[df['LF20_FVH'].isin(fvh_filter)]\n",
    "        df = df.drop(filtered.index, axis=0)\n",
    "\n",
    "    elif target == 'LF22_FVT':\n",
    "        developed_fvt = list(range(20,33)) + list(range(2901,2906)) # Agricultural/Developed classes\n",
    "        ag_fvt = [80, 81, 82] + list(range(2960, 2971))\n",
    "        fvt_filter = developed_fvt + ag_fvt\n",
    "        filtered = df.loc[df['LF20_FVT'].isin(fvt_filter)]\n",
    "        df = df.drop(filtered.index, axis=0)\n",
    "\n",
    "    # Join the filtered values together so they can be readded later\n",
    "    if target == 'LF22_F40':\n",
    "        dropped = pd.concat([null, filtered], axis=0)\n",
    "        dropped = dropped.rename(columns=column_dict[target])\n",
    "    else:\n",
    "        dropped = pd.concat([null, non_disturbed, filtered], axis=0)\n",
    "        dropped = dropped.rename(columns=column_dict[target])\n",
    "\n",
    "    \n",
    "    return df, dropped[pred_dict[target]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Read Function\n",
    "This function is used to read in chunks of the rull raster to be processed. The raster data is stored as blocks with height=1 and width=raster width, so we read in chunks composed of these blocks (e.g. 1000 blocks at a time). This corresponds to the row_slice argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_read(ras, row_slice):\n",
    "  \"\"\"\n",
    "  Reads in a subset (window) of the data to be processed.\n",
    "  Inputs:\n",
    "  - ras: Raster object read in using rasterio\n",
    "  - row_slice: Used to define the window height - in the form (row_start, row_end). \n",
    "\n",
    "  Returns:\n",
    "  - data: The data in the window as a 2D numpy array\n",
    "  - win: The Window object used to define the subset of the data. \n",
    "  - win_transform: The affine transform associated with the window. Used to update the metadata of the output of that chunk. \n",
    "  \"\"\"\n",
    "  \n",
    "  with rasterio.open(ras) as src:\n",
    "    col_slice = (0, src.width)  # Define row slice based on block size\n",
    "    win = Window.from_slices(row_slice, col_slice) \n",
    "    data = src.read(window=win)\n",
    "    win_transform = src.window_transform(win)\n",
    "    \n",
    "  return data, win, win_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Trained Model \n",
    "The model was not trained using null (-9999, -1111, -32768), non-disturbed, or agricultural/developed classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_raster(model, df, target):\n",
    "    \"\"\"\n",
    "    Predicts target class given a trained model and data to predict on.\n",
    "    Returns:\n",
    "    - A dataframe of predicted target values joined to the previously dropped values. \n",
    "    \"\"\"\n",
    "    # Specify output variable name as function of target\n",
    "    var_name_dict = {\n",
    "        \"LF22_F40\" : \"F40_Pred\",\n",
    "        \"LF22_FVC\" : \"FVC_Pred\",\n",
    "        \"LF22_FVH\" : \"FVH_Pred\",\n",
    "        \"LF22_FVT\" : \"FVT_Pred\",\n",
    "    }\n",
    "\n",
    "    # Prep the data - get the a clean dataframe and dropped observations\n",
    "    clean_df, dropped = predict_data_prep(df, target)\n",
    "\n",
    "    # Get list of predictors for run\n",
    "    predictors = list(model.feature_names_in_)\n",
    "    print(clean_df)\n",
    "    \n",
    "    X = clean_df[predictors].copy()\n",
    "\n",
    "    # Run model to predict\n",
    "    # If clean_df is empty, then all values were NULL and are in dropped\n",
    "    if clean_df.shape[0] > 0:\n",
    "        y_pred = model.predict(X)\n",
    "    else:\n",
    "        return dropped\n",
    "    \n",
    "    # Join the dropped observations back in \n",
    "    # This allows the result dataframe to be reshaped back to a raster\n",
    "    df = pd.DataFrame({var_name_dict[target] : y_pred},\n",
    "                       index=X.index)\n",
    "    df = pd.concat([dropped, df])\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    # Return predictions\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Model to Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_predict(row, target, model_fpath, win_height, raster_dict, ras_shape, out_dir, out_meta):\n",
    "\n",
    "    model = load(model_fpath)\n",
    "\n",
    "    row_start = row\n",
    "    row_end = row + win_height  # This is also the row_offset of the window\n",
    "\n",
    "    # make sure slice doesn't exceed row/col dims\n",
    "    if row_end > ras_shape[0]:\n",
    "        row_end = ras_shape[0]\n",
    "\n",
    "    # Define the window to be processed\n",
    "    row_slice = (row_start, row_end)\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    # For the current window, load data from each rasters\n",
    "    for var, fpath in raster_dict.items():\n",
    "        data_chunk, data_win, data_transform = windowed_read(fpath, row_slice)\n",
    "        data_dict[var] = data_chunk.ravel()\n",
    "\n",
    "    # Create a dataframe from the dictionary of datachunks\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # Look at the window currently processing\n",
    "    clear_output()\n",
    "\n",
    "    datetime = dt.datetime.now().strftime('%H-%M-%S-%f')\n",
    "    out_file = f\"data_chunk_{datetime}.tif\"\n",
    "    #print(f\"Row Slice: {row_slice}\")  \n",
    "    #print(f\"Writing {out_file}.\")\n",
    "    #print(f\"Processing window {i} of {floor(ras.shape[0] / win_height)}\")\n",
    "\n",
    "    # Run model to predict F40 Classes for window \n",
    "    out_arr = predict_raster(model, df, target)\n",
    "\n",
    "    # Reshape to 2D\n",
    "    out_arr_np = out_arr.to_numpy()\n",
    "    out_arr_2D = out_arr_np.reshape(data_chunk.shape)\n",
    "    out_arr_2D = out_arr_2D[0]\n",
    "\n",
    "    # update output metadata for chunk\n",
    "    out_meta.update({\n",
    "        'height': out_arr_2D.shape[0],\n",
    "        'width': out_arr_2D.shape[1],\n",
    "        'transform' : data_transform\n",
    "    })\n",
    "\n",
    "    # Write chunk out\n",
    "    datetime = dt.datetime.now().strftime('%H-%M-%S-%f')\n",
    "    out_file = f\"data_chunk_{datetime}.tif\"\n",
    "    with rasterio.open(os.path.join(out_dir, out_file), 'w+', **out_meta) as out:\n",
    "        out.write(out_arr_2D, indexes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model to predict raster\n",
    "Applies the model to chunks of the raster then mosaics those chunks, saves the resultant predicted raster, and returns the path to the saved raster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wrapper(target, raster_fpaths_dict, model_fpath):\n",
    "  raster_fpaths = raster_fpaths_dict\n",
    "\n",
    "  # Arbitrarily grab metadata from raster to use for updating output metdata\n",
    "  with rasterio.open(raster_fpaths['LF20_F40']) as src:\n",
    "    out_meta = src.meta.copy()\n",
    "\n",
    "  # Open a raster to access its attributes\n",
    "  ras = rasterio.open(raster_fpaths['LF20_F40'])\n",
    "\n",
    "  # Create directory using current datetime to output data chunks to\n",
    "  datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # Used to name output file\n",
    "  out_dir = make_dir(base_dir=out_chunk_dir, file_name=out_fname_dict[target])\n",
    "\n",
    "  # Define window height and iteration tracker\n",
    "  win_height = 1000  # Number of rows to process at once\n",
    "\n",
    "  Parallel(n_jobs=24)(delayed(window_predict)(row, target, model_fpath, win_height, raster_fpaths, ras.shape, out_dir, out_meta) for row in range(0, ras.shape[0], win_height))\n",
    "\n",
    "\n",
    "  ## Mosaic the data chunks\n",
    "  # Get the file paths of the generated data chunks\n",
    "  raster_fpaths = glob.glob(out_dir + \"/*.tif\")\n",
    "\n",
    "  # Get the rasterio dataset objects corresponding to each path\n",
    "  src_files_to_mosaic = []\n",
    "  for fpath in raster_fpaths:\n",
    "      src = rasterio.open(fpath)\n",
    "      src_files_to_mosaic.append(src)\n",
    "\n",
    "  # Merge the data chunks into a single raster\n",
    "  mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "\n",
    "  # Get the metadata for writing\n",
    "  out_meta = src.meta.copy()\n",
    "  out_meta.update({\n",
    "      \"driver\" : \"GTiff\",\n",
    "      \"height\" : mosaic.shape[1],\n",
    "      \"width\" : mosaic.shape[2],\n",
    "      \"transform\" : out_trans\n",
    "  })\n",
    "\n",
    "  # Write out the mosaic raster\n",
    "  fname = f\"{out_fname_dict[target] + \"_\" + datetime}.tif\"\n",
    "  with rasterio.open(os.path.join(out_raster_dir, fname), \"w\", **out_meta) as dest:\n",
    "      dest.write(mosaic)\n",
    "\n",
    "  print(f\"Raster written to {os.path.join(out_raster_dir, fname)}\")\n",
    "  return os.path.join(out_raster_dir, fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Main__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Models__\n",
    "Trains the models in the targets list, saves them, and stores the paths in model_fpaths_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LF22_FVC Model...\n",
      "LF22_FVC model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\\models\\LF22_FVC_HGBC_model_2024-05-29_12-06-42\n",
      "Fitting LF22_FVH Model...\n",
      "LF22_FVH model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\\models\\LF22_FVH_HGBC_model_2024-05-29_12-07-58\n",
      "Fitting LF22_F40 Model...\n",
      "LF22_F40 model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\\models\\LF22_F40_HGBC_model_2024-05-29_12-08-48\n"
     ]
    }
   ],
   "source": [
    "# Specify the models to train\n",
    "targets = [\"LF22_FVT\", \"LF22_FVC\", \"LF22_FVH\", \"LF22_F40\"]\n",
    "model_fpaths_dict = {}\n",
    "\n",
    "# Train the models\n",
    "for target in targets:\n",
    "    model_fpaths_dict[target] = train_model_wrapper(target, sample_points_fnames[target])\n",
    "\n",
    "print(model_fpaths_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Run Model Predictions__\n",
    "Generate predictions for FVT/FVC/FVH/F40. Predicted FVT is used as an input to predict FVC/FVH. Predicted FVT/FVC/FVH are used as inputs to predict F40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ! If using pre-trained models, specify them here (key=target, value=model path):\n",
    "#model_fpaths_dict = {}\n",
    "\n",
    "## Run models\n",
    "# Predict FVT\n",
    "pred_fvt_path = predict_wrapper('LF22_FVT',\n",
    "                                raster_fpaths_dict=raster_fpaths_dict,\n",
    "                                model_fpath=model_fpaths_dict['LF22_FVT'])\n",
    "\n",
    "# Save path to predicted FVT to dict\n",
    "raster_fpaths_dict['LF22_FVT'] = pred_fvt_path\n",
    "\n",
    "# Predict FVC/FVH\n",
    "targets = [\"LF22_FVH\", \"LF22_FVC\"]\n",
    "pred_fpaths = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"Processing {target}...\")\n",
    "    pred_fpaths[target] = predict_wrapper(target=target,\n",
    "                                          raster_fpaths_dict=raster_fpaths_dict,\n",
    "                                          model_fpath=model_fpaths_dict[target])\n",
    "    \n",
    "# Update raster_fpaths using the predicted FVC/FVH\n",
    "for raster, fpath in pred_fpaths.items():\n",
    "    raster_fpaths_dict[raster] = fpath\n",
    "\n",
    "# Predict F40\n",
    "print(raster_fpaths_dict)\n",
    "print(\"Processing F40...\")\n",
    "predict_wrapper(target=\"LF22_F40\",\n",
    "                raster_fpaths_dict=raster_fpaths_dict,\n",
    "                model_fpath=model_fpaths_dict[target])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
