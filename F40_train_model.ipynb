{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABOUT\n",
    "__Author__: Pat McCornack\n",
    "\n",
    "__Date__: 2/22/24\n",
    "\n",
    "__Purpose__: Train model on sample points to be applied to full raster extent. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path, listdir, mkdir\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths\n",
    "Use active_data_dir to switch between working off of the local machine or PNNL Drive. \n",
    "\n",
    "Update the paths_dict with appropriate directories and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_root_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\F40_modeling\"\n",
    "pnnl_root_dir = r\"\\\\pnl\\projects\\BPAWildfire\\data\\Landfire\\fuels_modeling\\F40_modeling\"\n",
    "\n",
    "# Define which data directory to work off of\n",
    "active_root_dir = local_root_dir\n",
    "\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "paths_dict = {\n",
    "    \"out_base_dir\" : os.path.join(local_root_dir, 'model_outputs/tabular'),  # Where to save outputs \n",
    "    \"new_dir_name\" : \"F40_model_results\",\n",
    "    \"ref_data_dir\" : os.path.join(active_root_dir, \"..\\LF_raster_data\\_tables\"),  # Location of LANDFIRE csvs (e.g. LF22_FVT_230.csv)\n",
    "    \"model_dir\" : os.path.join(active_root_dir, 'models'),  # Where to save model to \n",
    "    \"sample_points_dir\" : os.path.join(active_root_dir, \"data\\sample_points\"),  # Shapefile of sample points to train on \n",
    "    \"sample_points_fname\" : \"LF22_sample_points_2024-05-03.shp\",\n",
    "    \"model_fname\" : f\"LF22_F40_model_{datetime}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Functions__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Create a directory to output modeling results__\n",
    "Names the output directory using the datetime that the script was run. \n",
    "Returns the name of the directory. The returned directory is used to output the trained model and/or results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(base_dir, new_dir_name='model_results'):\n",
    "        \"\"\"\n",
    "        Create a directory named using the current datetime.\n",
    "        Input: base_dir - path to the directory where the new directory will be created. \n",
    "        Creates directory and returns string of path to that directory. \n",
    "        \"\"\"\n",
    "\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        output_dir = os.path.join(base_dir, new_dir_name + \"_\" + datetime)\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Pre-Process the Data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Dictionaries to Append Features\n",
    "Some features are separate attributes of the LANDFIRE dataset (e.g. BPS Fire Regime) and others are useful for results analysis (e.g. FDst attributes). These can be mapped to points using LANDFIRE CSVs. The below creates dictionaries to perform that mapping. \n",
    "\n",
    "This function is called by join_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref_data(ref_data_dir=paths_dict[\"ref_data_dir\"]):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of dictionaries of mappings between LANDFIRE raster values and other attributes associated with those values. \n",
    "\n",
    "    One example use case is using the BPS_NAME dictionary to map from the BPS layer value to the BPS_NAME attribute.\n",
    "    \"\"\"\n",
    "    data_dir = ref_data_dir\n",
    "    BPS_fname = \"LF20_BPS_220.csv\"\n",
    "    LF22_FDST_fname = \"LF22_FDST_230.csv\"\n",
    "\n",
    "    # Create empty dictionary\n",
    "    LF_ref_dicts = {}\n",
    "\n",
    "    # Get BPS reference dictionary\n",
    "    BPS_df = pd.read_csv(os.path.join(data_dir, BPS_fname))\n",
    "    LF_ref_dicts[\"BPS_NAME\"] = dict(BPS_df[['VALUE', 'BPS_NAME']].values)\n",
    "    LF_ref_dicts[\"BPS_FRG_NE\"] = dict(BPS_df[['VALUE', 'BPS_FRG_NEW']].values)\n",
    "\n",
    "    # Get FDST reference dictionaries \n",
    "    FDST_df = pd.read_csv(os.path.join(data_dir, LF22_FDST_fname ))\n",
    "    LF_ref_dicts[\"FDST_TYPE\"] = dict(FDST_df[['VALUE', 'D_TYPE']].values)\n",
    "    LF_ref_dicts[\"FDST_SEV\"] = dict(FDST_df[['VALUE', 'D_SEVERITY']].values)\n",
    "    LF_ref_dicts[\"FDST_TSD\"] = dict(FDST_df[['VALUE', 'D_TIME']].values)\n",
    "\n",
    "    return LF_ref_dicts\n",
    "                         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Features using Data Dictionaries\n",
    "Append in selected features using the LANDFIRE data dictionaries.\n",
    "\n",
    "__Note:__ Items in feature_list must be in the source_layers dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! This is used to analyze results\n",
    "def join_features(sample_points, feature_list = ['BPS_FRG_NE', 'FDST_TYPE', 'FDST_SEV', 'FDST_TSD']):\n",
    "    \"\"\"\n",
    "    Returns the sample_points layer with the features in feature_list appended. \n",
    "\n",
    "    Items in feature_list must be in the source_layers dictionary.  \n",
    "    \"\"\"\n",
    "    \n",
    "    LF_ref_dicts = read_ref_data()\n",
    "    \n",
    "    source_layers = {\n",
    "        'BPS_NAME' : 'BPS', \n",
    "        'BPS_FRG_NE' : 'BPS',\n",
    "        'FDST_TYPE' : 'LF22_FDST',\n",
    "        'FDST_SEV' : 'LF22_FDST', \n",
    "        'FDST_TSD' : 'LF22_FDST'\n",
    "    }\n",
    "\n",
    "    for feature in feature_list:\n",
    "        sample_points[feature] = sample_points[source_layers[feature]].map(LF_ref_dicts[feature]).copy()\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Sample Points\n",
    "Before training the model, the sample points need to be filtered. Filtering steps are:\n",
    "- Remove Null points (-9999/-1111 values) - these will not be updated in the final raster. \n",
    "- Remove non-burnable F40 classes - We assume that these are static and will not be updated in the final raster. \n",
    "- Optionally, join in additional features. feature_list may be modified to select which features to add in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(sample_point_fpath, join_features=False):\n",
    "    \"\"\"\n",
    "    Reads in sample points and prepares the data for the model.\n",
    "    sample_point_fpath: Path to the sample points to be used to train the model\n",
    "    Returns processed sample points. \n",
    "    \"\"\"\n",
    "    # Read in gdf\n",
    "    sample_points = gpd.read_file(sample_point_fpath)\n",
    "\n",
    "    # Drop unneeeded columns if present\n",
    "    sample_points = sample_points.drop(['Classified', 'GrndTruth', 'NEAR_FID', 'NEAR_DIST'], axis=1,\n",
    "                                       errors='ignore')\n",
    "\n",
    "    # Remove observations with -9999/-1111 in any field \n",
    "    matches = sample_points[(sample_points.isin([-1111, -9999])).any(axis=1)]  # Find rows with -1111/-9999 in any column\n",
    "    sample_points = sample_points.drop(matches.index, axis=0)  # Drop those rows\n",
    "\n",
    "    # Remove Non-Burnable Classes\n",
    "    F40_NB = [91, 92, 93, 98, 99]  # Nonburnable F40 Classes\n",
    "    sample_points = sample_points.loc[~sample_points['LF22_F40'].isin(F40_NB)]  # Drop NB classes\n",
    "\n",
    "    # Join in additional features if specified\n",
    "    if join_features == True:\n",
    "        sample_points = join_features(sample_points, feature_list=['BPS_NAME', 'BPS_FRG_NE' 'FDST_TYPE', 'FDST_SEV', 'FDST_TSD'])\n",
    "    \n",
    "    return sample_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Model__\n",
    "Trains and returns HGBC model provided data. Subsets the training data to a provided list of predictors and predicts the specified target feature.\n",
    "\n",
    "__Note__: Aspect, Elevation, and Slope are the only continuous LANDFIRE datasets, therefore any feature not in that list is assumed to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, predictors, target='LF22_F40', seed=1234):\n",
    "    \"\"\"\n",
    "    Returns trained model of specified type given a set of predictors. \n",
    "    \"\"\"\n",
    "    # Set parameters for model\n",
    "    class_weight = \"balanced\"\n",
    "    learning_rate = 0.1\n",
    "    max_iter = 100\n",
    "\n",
    "    print(f\"Predictors: {predictors}\")\n",
    "    print(f\"Target: {target}\")\n",
    "\n",
    "    # Get list of categorical variables to encode\n",
    "    cat_variables = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]\n",
    "\n",
    "    # Separate training data predictors/response\n",
    "    y_train = train_data[target].copy()\n",
    "    X_train = train_data[predictors].copy()\n",
    "\n",
    "    # Fit specified classifier with training data\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        categorical_features=cat_variables,\n",
    "        class_weight=class_weight,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=max_iter,\n",
    "        random_state=seed)\n",
    "    \n",
    "    # Fit the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the trained model\n",
    "    return model   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluate Model__\n",
    "Provided a trained model, test data, a set of predictors, and a target - return a list of predictions metrics on prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_data, predictors, target='LF22_F40'):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing 1. 'metrics' : a list of metrics quantifying model performance, and 2. 'predictions' : a list of predictions corresponding to \n",
    "    each observation in the sample_points data.\n",
    "    \"\"\" \n",
    "    # Separate the predictors from target\n",
    "    y_test = test_data[target].copy()\n",
    "    X_test = test_data[predictors].copy()\n",
    "\n",
    "    # Run model to predict target\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Get metrics\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    bal_acc = round(balanced_accuracy_score(y_test, y_pred),3)\n",
    "    recall = round(recall_score(y_test, y_pred, average='macro'), 3)\n",
    "    precision = round(precision_score(y_test, y_pred, average='macro'), 3)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='macro'), 3)\n",
    "\n",
    "    print(f\"Predictors: {predictors}\")\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics\" : [accuracy, bal_acc, recall, precision, f1, predictors],\n",
    "        \"predictions\" : y_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Main__\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Model__\n",
    "Update the paths in paths_dict, if necessary, then run to train and save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "# Define path to sample points to train on \n",
    "sample_point_fpath = os.path.join(paths_dict['sample_points_dir', paths_dict['sample_points_fname']])\n",
    "\n",
    "# Define target and predictors\n",
    "target = 'LF22_F40'\n",
    "predictors = ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'ZONE', 'BPS_FRG_NE']\n",
    "\n",
    "# Read in and prepare the data\n",
    "sample_points = data_prep(sample_point_fpath, predictors)\n",
    "\n",
    "# Train the model\n",
    "model = train_model(sample_points, predictors)\n",
    "\n",
    "# Save the model\n",
    "dump(model, os.path.join(paths_dict['model_dir'], paths_dict['model_fname']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing using FVT Predictions for F40 Model\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Baseline__\n",
    "First check the overall performance of the F40 model using LANDFIRE data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'ZONE', 'BPS_FRG_NE']\n",
      "Target: LF22_F40\n",
      "Score on the test set: \n",
      "Predictors: ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'ZONE', 'BPS_FRG_NE']\n",
      "Target: LF22_F40\n",
      "Accuracy: 0.985\n",
      "Balanced Accuracy: 0.953\n",
      "Recall: 0.953\n",
      "Precision: 0.938\n",
      "F1: 0.94\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "model_type = 'HGBC'\n",
    "\n",
    "# Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "# Define set of predictors and target\n",
    "predictors = ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'ZONE', 'BPS_FRG_NE']\n",
    "target = 'LF22_F40'\n",
    "\n",
    "# Read in and prepare the data\n",
    "sample_points = data_prep(os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname']), predictors)\n",
    "\n",
    "# Perform train/test split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_points, predictors)\n",
    "\n",
    "# Get the model score\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test_points, predictors=predictors, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\mcco573\\\\OneDrive - PNNL\\\\Documents\\\\_Projects\\\\BPA Wildfire\\\\F40_modeling\\\\model_outputs/tabular\\\\F40_model_results_2024-05-10_11-17-36\\\\LF22_F40_HGBC_model_2024-05-10_11-17-37']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test_points set\n",
    "test_points[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=paths_dict['new_dir_name'])\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test_points.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  __Baseline -- Disturbed Areas Only__\n",
    "Check how the F40 model performs with LANDFIRE data on only disturbed points and official LANDFIRE data. \n",
    "Note that this uses the same model trained in 'Baseline' above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to geodatabase and layer of sample points  - this is the test set\n",
    "data_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\\data\\sample_points\"\n",
    "test_sample_point_fpath = 'sample_points_4-17-24_200k_Disturbed.shp'\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'LF22_F40', 'ZONE', 'BPS_FRG_NE']\n",
    "\n",
    "# Read in and prepare the data\n",
    "test = data_prep(os.path.join(data_dir, test_sample_point_fpath), predictors)\n",
    "\n",
    "target = 'LF22_F40'\n",
    "model_type = 'HGBC'\n",
    "\n",
    "# Get the model score\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, attributes=predictors, model_type='HGBC',target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Using Predicted LF22 FVT__\n",
    "A raster of predicted LF22 FVT values for all disturbed points has been generated using LF20 data. This raster has been uploaded to the sample points as PRED_FVT. The goal here is to assess whether the accuracy changes when using the predicted FVT data rather than the LANDFIRE FVT data. \n",
    "\n",
    "----- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to geodatabase and layer of sample points  - this is the test set\n",
    "data_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\\data\\sample_points\"\n",
    "test_sample_point_fpath = 'sample_points_4-17-24_200k_Disturbed.shp'\n",
    "\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'LF22_F40', 'ZONE', 'BPS_FRG_NE']\n",
    "\n",
    "# Read in and prepare the data\n",
    "test = data_prep(os.path.join(data_dir, test_sample_point_fpath), predictors)\n",
    "test = test.rename(columns = {\n",
    "    'LF22_FVT' : 'Original_LF22_FVT',\n",
    "    'PRED_FVT' : 'LF22_FVT'  # Rename prediction column in order to run model. \n",
    "})\n",
    "\n",
    "target = 'LF22_F40'\n",
    "model_type = 'HGBC'\n",
    "\n",
    "\n",
    "# Get the model score\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, attributes=predictors, model_type='HGBC',target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test set\n",
    "test[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'])\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\F40 Random Forest Model\\models\"\n",
    "model_fname = f\"{target}_model_{datetime}\"\n",
    "dump(model, os.path.join(model_dir, model_fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
