{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "\n",
    "__Author__: Pat McCornack\n",
    "\n",
    "__Date__: 4/9/24\n",
    "\n",
    "__Purpose__: Train model to update FVT using most recent FDist data. \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from joblib import dump, load\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Paths\n",
    "Define set of filepaths to conveniently switch between working off local files or the PNNL drive. Set active_data_dir to either local_data_dir or pnnl_data_dir depending on which you're working off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_root_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\"\n",
    "pnnl_root_dir = r\"\\\\pnl\\projects\\BPAWildfire\\data\\Landfire\\fuels_modeling\\fuelscape_modeling\"\n",
    "\n",
    "# Define which data directory to work off of\n",
    "active_root_dir = local_root_dir\n",
    "\n",
    "paths_dict = {\n",
    "    \"out_base_dir\" : os.path.join(active_root_dir, r\"model_outputs\\tabular\"),  # Where to save result outputs \n",
    "    \"ref_data_dir\" : os.path.join(active_root_dir, r\"..\\LF_raster_data\\_tables\"),  # Location of LF csvs (e.g. LF22_FVT_230.csv)\n",
    "    \"sample_points_dir\" : os.path.join(active_root_dir, r\"sample_points\"),  # Location of shapefile to train model on\n",
    "    \"model_dir\" : os.path.join(active_root_dir, \"models\")\n",
    "}\n",
    "\n",
    "sample_points_fnames = {\n",
    "    \"LF22_F40\" : \"LF22_F40_sample_points_2024-05-29_100k.shp\",\n",
    "    \"LF22_FVT\" : \"LF22_FVT_Disturbed_sample_points_2024-05-29_200k.shp\",\n",
    "    \"LF22_FVC\" : \"LF22_FVT_Disturbed_sample_points_2024-05-29_200k.shp\",\n",
    "    \"LF22_FVH\" : \"LF22_FVT_Disturbed_sample_points_2024-05-29_200k.shp\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Functions__\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Create a Directory to Output Modeling Results__\n",
    "Names the output directory using the datetime that the script was run. \n",
    "Returns the name of the directory. The returned directory is used to output the trained model and/or results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(base_dir, new_dir_name='model_results'):\n",
    "        \"\"\"\n",
    "        Returns path to a directory created at the specified base_dir location. \n",
    "\n",
    "        The name of the created directory can optionally be specified using the dir_name argument. \n",
    "        \"\"\"\n",
    "\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        output_dir = os.path.join(base_dir, new_dir_name + \"_\" + datetime)\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Pre-Process the Data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Create Data Dictionaries to Append Features__\n",
    "Some features are separate attributes of the LANDFIRE dataset (e.g. BPS Fire Regime) and others are useful for results analysis (e.g. FDst attributes). These can be mapped to points using LANDFIRE CSVs. The below creates dictionaries to perform that mapping. \n",
    "\n",
    "This function is called by join_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_read_ref_data(ref_data_dir=paths_dict[\"ref_data_dir\"]):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of dictionaries of mappings between LANDFIRE raster values and other attributes associated with those values. \n",
    "    \"\"\"\n",
    "    data_dir = ref_data_dir\n",
    "    BPS_fname = \"LF20_BPS_220.csv\"\n",
    "   \n",
    "    # Create empty dictionary\n",
    "    LF_ref_dicts = {}\n",
    "\n",
    "    # Get BPS reference dictionary\n",
    "    BPS_df = pd.read_csv(os.path.join(data_dir, BPS_fname))\n",
    "    LF_ref_dicts[\"BPS_NAME\"] = dict(BPS_df[['VALUE', 'BPS_NAME']].values)\n",
    "    LF_ref_dicts[\"BPS_FRG_NE\"] = dict(BPS_df[['VALUE', 'FRG_NEW']].values)\n",
    "\n",
    "    return LF_ref_dicts\n",
    "                         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Append Features using Data Dictionaries__\n",
    "Append in selected features using the LANDFIRE data dictionaries.\n",
    "\n",
    "__Note:__ Items in feature_list must be in the source_layers dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_join_features(sample_points, feature_list = ['BPS_NAME']):\n",
    "    \"\"\"\n",
    "    Returns the sample_points layer with the features in feature_list appended. \n",
    "\n",
    "    Items in feature_list must be in the source_layers dictionary.  \n",
    "    \"\"\"\n",
    "    \n",
    "    LF_ref_dicts = read_ref_data()\n",
    "    \n",
    "    source_layers = {\n",
    "        'BPS_NAME' : 'BPS',\n",
    "        'BPS_FRG_NE' : 'BPS',\n",
    "        'FDST_TYPE' : 'LF22_FDST',\n",
    "        'FDST_SEV' : 'LF22_FDST', \n",
    "        'FDST_TSD' : 'LF22_FDST'\n",
    "    }\n",
    "\n",
    "    # Iterate through feature_list and append features to sample_points\n",
    "    for feature in feature_list:\n",
    "        sample_points[feature] = sample_points[source_layers[feature]].map(LF_ref_dicts[feature]).copy()\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Low Count Filter__\n",
    "Some classes have exceedingly low representation in the dataset. In order to run train_test_split, the target class must have a count greater than 1. The following will filter out classes with counts of 1. Given the size of the dataset excluding these classes will not detrimentally impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_count_filter(sample_points, target):  \n",
    "    \"\"\"\n",
    "    Returns modified sample_points dataframe. Classes in the target feature that have counts of 1 are removed. \n",
    "    \"\"\"  \n",
    "    # Group by FVT\n",
    "    obs_counts = sample_points.groupby(target).count()\n",
    "\n",
    "    # Identify FVTs with low observation counts\n",
    "    low_count_classes = obs_counts[obs_counts.iloc[:,0] < 5].index.tolist()\n",
    "\n",
    "    # Remove those FVTs from sample_points\n",
    "    sample_points = sample_points.loc[~sample_points[target].isin(low_count_classes)]\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Prepare the Sample Points__\n",
    "Before training the model, the sample points need to be filtered. Filtering steps are:\n",
    "- Remove near points - defined as points that are less than 70m apart.\n",
    "- Remove Null points (-9999/-1111 values) - these will not be updated in the final raster. \n",
    "- Remove undisturbed values - the model is intended to update points that have been disturbed in the last 10 years. \n",
    "- Remove points with agricultural or developed FVT classes - these are updated using datasets not used for other classes. \n",
    "- Remove observations with classes that have very low representation (less than 5 observations in the class) in the dataset. This is done to allow train_test_split to run. \n",
    "- Optionally, join in additional features. feature_list may be modified to select which features to add in (see join_features for further details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_prep(sample_point_fpath, target):\n",
    "    \"\"\"\n",
    "    Returns dataframe of processed sample points. Sample points are read in from shapefile. Processing includes filtering out null points and developed/agricultural FVTs as \n",
    "    well as appending additional features. \n",
    "    \"\"\"\n",
    "\n",
    "    # Read in gdf\n",
    "    sample_points = gpd.read_file(sample_point_fpath)\n",
    "\n",
    "    # Filter out near points\n",
    "    sample_points = sample_points.loc[sample_points['NEAR_FID'] == -1]\n",
    "\n",
    "    # Drop unneeeded columns if present\n",
    "    sample_points = sample_points.drop(['Classified', 'GrndTruth', 'NEAR_FID', 'NEAR_DIST'], axis=1,\n",
    "                                       errors='ignore')\n",
    "    \n",
    "    # Remove observations with -9999/-1111 in any field \n",
    "    sample_points = sample_points.loc[~sample_points.isin([-1111, -9999]).any(axis=1)]\n",
    "\n",
    "    # Join in additional features\n",
    "    sample_points = join_features(sample_points)\n",
    "\n",
    "    # Drop non-disturbed values if not predicting F40\n",
    "    if target != 'LF22_F40':     \n",
    "        # Filter out points that weren't disturbed\n",
    "        sample_points = sample_points.loc[sample_points['LF22_FDST'] != 0]\n",
    "\n",
    "    # Filter classes based on target\n",
    "    if target == 'LF22_F40':\n",
    "        F40_NB = [91, 93]\n",
    "        sample_points = sample_points.loc[~sample_points['LF22_F40'].isin(F40_NB)]\n",
    "\n",
    "    elif target == 'LF22_FVT':\n",
    "        developed_fvt = list(range(20,33)) + list(range(2901,2906))\n",
    "        ag_fvt = [80, 81, 82] + list(range(2960, 2971))\n",
    "        fvt_filter = developed_fvt + ag_fvt\n",
    "        sample_points = sample_points.loc[~sample_points['LF22_FVT'].isin(fvt_filter)]\n",
    "\n",
    "    elif target == 'LF22_FVH' or target == 'LF22_FVC':\n",
    "        filter = list(range(20, 70)) + list(range(80, 86))\n",
    "        sample_points = sample_points.loc[~sample_points[target].isin(filter)]\n",
    "\n",
    "    # Filter out classes in the target feature with very low counts\n",
    "    sample_points = low_count_filter(sample_points, target)\n",
    "\n",
    "    return sample_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Instantiate Estimators__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Histogram Based Gradient Boosting Classifier__\n",
    "Scikit-learn implementation of Histogram Based Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histGradientBoostingClassifier(categorical_feature_list, class_weight='balanced', seed=1234):\n",
    "    \"\"\"\n",
    "    Returns specified histogram-based gradient boosting classifier. \n",
    "    \"\"\"\n",
    "\n",
    "    hgb_classifier = HistGradientBoostingClassifier(\n",
    "        categorical_features=categorical_feature_list,  # Natively handle categorical variables\n",
    "        class_weight=class_weight,\n",
    "        random_state=seed,\n",
    "        learning_rate=0.01,\n",
    "        max_iter=100\n",
    "    )\n",
    "\n",
    "    return hgb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Model__\n",
    "Trains and returns HGBC model provided data. Subsets the training data to a provided list of predictors and predicts the specified target feature.\n",
    "\n",
    "__Note__: Aspect, Elevation, and Slope are the only continuous LANDFIRE datasets, therefore any feature not in that list is assumed to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train_data, predictors, target):\n",
    "    \"\"\"\n",
    "    Returns trained model of specified type given a set of predictors. \n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = \"balanced\"\n",
    "\n",
    "    # Get list of predictors for run\n",
    "    cat_features = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]  # Categorical features for HGBC models  \n",
    "    \n",
    "    # Separate training data predictors/response\n",
    "    y_train = train_data[target].copy()\n",
    "    X_train = train_data[predictors].copy()\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = histGradientBoostingClassifier(categorical_feature_list=cat_features, class_weight=class_weight)\n",
    "      \n",
    "    # Fit the model with the training data\n",
    "    print(f'Fitting {target} Model...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the fit model\n",
    "    return model         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluate Model__\n",
    "Provided a trained model, test data, a set of predictors, and a target - return a list of predictions metrics on prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_data, predictors, target):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing 1. 'metrics' : a list of metrics quantifying model performance, and 2. 'predictions' : a list of predictions corresponding to \n",
    "    each observation in the sample_points data.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Separate the predictors from target\n",
    "    y_test = test_data[target].copy()\n",
    "    x_test = test_data[predictors].copy()\n",
    "\n",
    "    # Perform prediction\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Get metrics\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    bal_acc = round(balanced_accuracy_score(y_test, y_pred),3)\n",
    "    recall = round(recall_score(y_test, y_pred, average='macro'), 3)\n",
    "    precision = round(precision_score(y_test, y_pred, average='macro'), 3)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='macro'), 3)\n",
    "\n",
    "    print(f\"Predictors: {predictors}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics\" : [accuracy, bal_acc, recall, precision, f1, predictors],\n",
    "        \"predictions\" : y_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_wrapper(target, sample_points_fname):\n",
    "    predictors_dict = {\n",
    "        'LF22_FVT' : ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH'],\n",
    "        'LF22_FVC' : ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "        'LF22_FVH' : ['LF22_FVT', 'LF20_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "        'LF22_F40' : ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'ZONE', 'BPS_FRG_NE']\n",
    "    }\n",
    "\n",
    "    # Define path to sample points\n",
    "    sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], sample_points_fname)\n",
    "\n",
    "    # Read in and prepare data\n",
    "    sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "    # Train model\n",
    "    model = fit_model(train_data=sample_points, predictors=predictors_dict[target], target=target)\n",
    "\n",
    "    # Save model \n",
    "    datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    model_fname = f\"{target}_HGBC_model_{datetime}\"\n",
    "    model_fpath = os.path.join(paths_dict['model_dir'], model_fname)\n",
    "    dump(model, model_fpath)\n",
    "    print(f\"{target} model written to: {model_fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Main__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Specify the models to train by adjusting the 'targets' list, then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LF22_FVC Model...\n",
      "LF22_FVC model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\\models\\LF22_FVC_HGBC_model_2024-05-29_12-06-42\n",
      "Fitting LF22_FVH Model...\n",
      "LF22_FVH model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\\models\\LF22_FVH_HGBC_model_2024-05-29_12-07-58\n",
      "Fitting LF22_F40 Model...\n",
      "LF22_F40 model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\fuelscape_modeling\\models\\LF22_F40_HGBC_model_2024-05-29_12-08-48\n"
     ]
    }
   ],
   "source": [
    "# Specify the models to train\n",
    "targets = [\"LF22_FVC\", \"LF22_FVH\", \"LF22_F40\"]\n",
    "\n",
    "# Train the models\n",
    "for target in targets:\n",
    "    train_model_wrapper(target, sample_points_fnames[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quick Assessment\n",
    "Get performance metrics for a given model.\n",
    "Set the target variable by changing 'target' to LF22_FVT/LF22_FVC/LF22_FVH/LF22_F40 and run the celll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LF22_FVT\n",
      "Score on the Training Set: \n",
      "Predictors: ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
      "Accuracy: 0.996\n",
      "Balanced Accuracy: 0.998\n",
      "Recall: 0.998\n",
      "Precision: 0.989\n",
      "F1: 0.993\n",
      "Score on the test set: \n",
      "Predictors: ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
      "Accuracy: 0.996\n",
      "Balanced Accuracy: 0.998\n",
      "Recall: 0.998\n",
      "Precision: 0.992\n",
      "F1: 0.995\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "\n",
    "# Define the target model to assess\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "# Dictionary to define predictors based on target\n",
    "predictors_dict = {\n",
    "    'LF22_FVT' : ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH'],\n",
    "    'LF22_FVC' : ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "    'LF22_FVH' : ['LF22_FVT', 'LF20_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "    'LF22_F40' : ['LF22_FVT', 'LF22_FVH', 'LF22_FVC', 'LF22_FDST', 'ZONE', 'BPS_FRG_NE']\n",
    "}\n",
    "\n",
    "# Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "# Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], sample_points_fnames[target])\n",
    "\n",
    "# Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "# Perform Train/Test Split\n",
    "train, test = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])\n",
    "\n",
    "# Train model\n",
    "model = fit_model(train_data=train, predictors=predictors_dict[target], target=target)\n",
    "\n",
    "# Evaluate Model\n",
    "print(target)\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors_dict[target], target=target)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors_dict[target], target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "model_type = 'HGBC'\n",
    "## Concatenate predictions to test set\n",
    "test[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=f'{target}_model_results')\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
