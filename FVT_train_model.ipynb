{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "\n",
    "__Author__: Pat McCornack\n",
    "\n",
    "__Date__: 4/9/24\n",
    "\n",
    "__Purpose__: Develop model to update FVT using most recent FDist data. \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from joblib import dump\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Paths\n",
    "Define set of filepaths to conveniently switch between working off local files or the PNNL drive. Set active_data_dir to either local_data_dir or pnnl_data_dir depending on which you're working off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_root_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\"\n",
    "pnnl_root_dir = r\"\\\\pnl\\projects\\BPAWildfire\\data\\Landfire\\fuels_modeling\\Fuel Attributes Model\"\n",
    "\n",
    "# Define which data directory to work off of\n",
    "active_root_dir = local_root_dir\n",
    "\n",
    "paths_dict = {\n",
    "    \"out_base_dir\" : os.path.join(active_root_dir, r\"model_outputs\\tabular\"),  # Where to save outputs \n",
    "    \"ref_data_dir\" : os.path.join(active_root_dir, r\"..\\LF_raster_data\\_tables\"),  # Location of LF csvs (e.g. LF22_FVT_230.csv)\n",
    "    \"sample_points_dir\" : os.path.join(active_root_dir, r\"data\\sample_points\"),  # Location of shapefile to train model on\n",
    "    \"sample_points_fname\" : \"sample_points_4-17-24_200k_Disturbed.shp\",  # Name of shapefile to train model on\n",
    "    \"runs_dict_fpath\" : os.path.join(active_root_dir, r\"data\\runs_dict.csv\"),  # Used to test different sets of predictors \n",
    "    \"model_dir\" : os.path.join(active_root_dir, \"models\")\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Functions__\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Create a Directory to Output Modeling Results__\n",
    "Names the output directory using the datetime that the script was run. \n",
    "Returns the name of the directory. The returned directory is used to output the trained model and/or results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(base_dir, new_dir_name='model_results'):\n",
    "        \"\"\"\n",
    "        Returns path to a directory created at the specified base_dir location. \n",
    "\n",
    "        The name of the created directory can optionally be specified using the dir_name argument. \n",
    "        \"\"\"\n",
    "\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        output_dir = os.path.join(base_dir, new_dir_name + \"_\" + datetime)\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Categorical Variable Encoder__\n",
    "Categorical variables must be encoded before being used in some model types. Note that due to HGBC's native encoding support this function is not needed when using that model type.\n",
    "\n",
    "__Note:__ Aspect, elevation, and slope are the only LANDFIRE variables of interest that are not categorical, which is why they're removed from the predictor list before the data is encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targetEncoder(train, test, predictors, target):\n",
    "    \"\"\"\n",
    "    Returns the train and test sets with the categorical predictors encoded. The predictors argument is a list of features being used as \n",
    "    predictors in the model, while target is the name of target feature. \n",
    "    \"\"\"\n",
    "    # Subset to attributes of interest\n",
    "    attributes = predictors + [target]\n",
    "    train = train[attributes]\n",
    "    test = test[attributes]\n",
    "\n",
    "    # Separate out the predictors from the target and remove continuous variables\n",
    "    cat_predictors = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]  # Drops continuous variables\n",
    "\n",
    "    # Encode the features\n",
    "    enc = TargetEncoder(target_type=\"multiclass\", random_state=1234).set_output(transform=\"pandas\")\n",
    "    enc.fit(train[cat_predictors], train[target])  # Fit the encoder\n",
    "    train_trans = enc.transform(train[cat_predictors])  # Encode the train data\n",
    "    test_trans = enc.transform(test[cat_predictors])  # Encode the test data\n",
    "\n",
    "    # Replace the features with encoded features\n",
    "    train = train.drop(cat_predictors, axis=1)\n",
    "    train = pd.concat([train, train_trans], axis=1)\n",
    "\n",
    "    test = test.drop(cat_predictors, axis=1)\n",
    "    test = pd.concat([test, test_trans], axis=1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Standardize Data__\n",
    "This is primarily used to standardize data for the MLP since tree-based methods aren't sensitive to data scale. By default it is set to standardize only topographic variables given that they are the only continuous LANDFIRE features of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(train, test, features=['SLOPE', 'ASPECT', 'ELEVATION']):\n",
    "    \"\"\"\n",
    "    Return train and test datasets with the features in the 'features' argument having been standardized.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the scaler to the training data\n",
    "    scaler = StandardScaler().fit(train[features])\n",
    "\n",
    "    # Scale the training data\n",
    "    train[features] = scaler.transform(train[features])\n",
    "\n",
    "    # Scale the test data\n",
    "    test[features] = scaler.transform(test[features])\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Get List of Lists of Predictor Sets__\n",
    "In the interest of assessing model performance using multiple sets of predictors, this function utilizes an excel file to generate a list of lists of sets of predictors that can be iterated through the model train/evaluation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runs(runs_dict_fpath=paths_dict['runs_dict_fpath']):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, where each list is a set of predictors to evaluate model performance with. \n",
    "\n",
    "    runs_dict_fpath is the path to an excel file where each column corresponds to a feature, and each row corresponds to a set\n",
    "    of predictors. If a cell contains a '1', then that feature is included that set of predictors.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(runs_dict_fpath)\n",
    "    \n",
    "    # Drop the \"Run\" column\n",
    "    df = df.drop('Run', axis=1)\n",
    "    \n",
    "    runs = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        runs.append(row.loc[row==1].index.tolist())\n",
    "    \n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate Combinations of Features of Interest__\n",
    "Alternatively, automatically generate all possible combinations of a set of potential predictor features to evaluate. \n",
    "\n",
    "__Caution__: The size of the list of lists of predictor sets can quickly become unmanageable when too many features of interest are included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(attributes, target):\n",
    "    \"\"\"\n",
    "    Returns a list of lists of all possible combinations of the list of attributes provided. \n",
    "    \"\"\"\n",
    "\n",
    "    attributes = [x for x in attributes if x not in [target]]  # Drop the target variable\n",
    "    predictors = []\n",
    "\n",
    "    for i in range(len(attributes)):\n",
    "        combs = [list(x) for x in combinations(attributes, i)]\n",
    "        for item in combs:\n",
    "            predictors.append(item)\n",
    "\n",
    "    return predictors[1:]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Pre-Process the Data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Create Data Dictionaries to Append Features__\n",
    "Some features are separate attributes of the LANDFIRE dataset (e.g. BPS Fire Regime) and others are useful for results analysis (e.g. FDst attributes). These can be mapped to points using LANDFIRE CSVs. The below creates dictionaries to perform that mapping. \n",
    "\n",
    "This function is called by join_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref_data(ref_data_dir=paths_dict[\"ref_data_dir\"]):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of dictionaries of mappings between LANDFIRE raster values and other attributes associated with those values. \n",
    "    \"\"\"\n",
    "    data_dir = ref_data_dir\n",
    "    BPS_fname = \"LF20_BPS_220.csv\"\n",
    "    LF16_EVT_fname = \"LF16_EVT_200.csv\"\n",
    "    LF22_FDST_fname = \"LF22_FDST_230.csv\"\n",
    "    LF22_FVT_fname = \"LF22_FVT_230.csv\"\n",
    "    LF22_EVT_fname = \"LF22_EVT_230.csv\"\n",
    "    \n",
    "    # Create empty dictionary\n",
    "    LF_ref_dicts = {}\n",
    "\n",
    "    # Get BPS reference dictionary\n",
    "    BPS_df = pd.read_csv(os.path.join(data_dir, BPS_fname))\n",
    "    LF_ref_dicts[\"BPS_NAME\"] = dict(BPS_df[['VALUE', 'BPS_NAME']].values)\n",
    "    LF_ref_dicts[\"BPS_FRG\"] = dict(BPS_df[['VALUE', 'FRG_NEW']].values)\n",
    "\n",
    "    # Get FDST reference dictionaries \n",
    "    FDST_df = pd.read_csv(os.path.join(data_dir, LF22_FDST_fname ))\n",
    "    LF_ref_dicts[\"FDST_TYPE\"] = dict(FDST_df[['VALUE', 'D_TYPE']].values)\n",
    "    LF_ref_dicts[\"FDST_SEV\"] = dict(FDST_df[['VALUE', 'D_SEVERITY']].values)\n",
    "    LF_ref_dicts[\"FDST_TSD\"] = dict(FDST_df[['VALUE', 'D_TIME']].values)\n",
    "\n",
    "    # Get EVT reference dictionaries\n",
    "    EVT_df = pd.read_csv(os.path.join(data_dir, LF16_EVT_fname))\n",
    "    LF_ref_dicts[\"EVT_PHYS\"] = dict(EVT_df[['VALUE', 'EVT_PHYS']].values)\n",
    "    LF_ref_dicts[\"EVT_GP_N\"] = dict(EVT_df[['VALUE', 'EVT_GP_N']].values) \n",
    "    LF_ref_dicts[\"EVT_CLASS\"] = dict(EVT_df[['VALUE', 'EVT_CLASS']].values) \n",
    "\n",
    "    # Get FVT reference dictionaries\n",
    "    LF22_FVT_df = pd.read_csv(os.path.join(data_dir, LF22_FVT_fname))\n",
    "    LF_ref_dicts['LF22_FVT'] = dict(LF22_FVT_df[['VALUE', 'EVT_FUEL_N']].values)\n",
    "    \n",
    "    ## Map FVT to EVT Groupings\n",
    "    LF22_EVT_df = pd.read_csv(os.path.join(data_dir, LF22_EVT_fname))\n",
    "    LF_ref_dicts['FVT_EVT'] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_FUEL_N']].values)\n",
    "    LF_ref_dicts[\"FVT_PHYS\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_PHYS']].values)\n",
    "    LF_ref_dicts[\"FVT_GP_N\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_GP_N']].values) \n",
    "    LF_ref_dicts[\"FVT_CLASS\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_CLASS']].values) \n",
    "    LF_ref_dicts[\"FVT_LF\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_LF']].values)\n",
    "\n",
    "    return LF_ref_dicts\n",
    "                         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Append Features using Data Dictionaries__\n",
    "Append in selected features using the LANDFIRE data dictionaries.\n",
    "\n",
    "__Note:__ Items in feature_list must be in the source_layers dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(sample_points, feature_list = ['BPS_NAME', 'BPS_FRG', 'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N', 'EVT_CLASS']):\n",
    "    \"\"\"\n",
    "    Returns the sample_points layer with the features in feature_list appended. \n",
    "\n",
    "    Items in feature_list must be in the source_layers dictionary.  \n",
    "    \"\"\"\n",
    "    \n",
    "    LF_ref_dicts = read_ref_data()\n",
    "    \n",
    "    source_layers = {\n",
    "        'BPS_NAME' : 'BPS', \n",
    "        'BPS_FRG' : 'BPS',\n",
    "        'FDST_TYPE' : 'LF22_FDST',\n",
    "        'FDST_SEV' : 'LF22_FDST', \n",
    "        'FDST_TSD' : 'LF22_FDST', \n",
    "        'EVT_PHYS' : 'LF16_EVT',\n",
    "        'EVT_GP_N' : 'LF16_EVT',\n",
    "        'EVT_CLASS' : 'LF16_EVT'\n",
    "    }\n",
    "\n",
    "    # Iterate through feature_list and append features to sample_points\n",
    "    for feature in feature_list:\n",
    "        sample_points[feature] = sample_points[source_layers[feature]].map(LF_ref_dicts[feature]).copy()\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Low Count Filter__\n",
    "Some classes have exceedingly low representation in the dataset. In order to run train_test_split, the target class must have a count greater than 1. The following will filter out classes with counts of 1. Given the size of the dataset excluding these classes will not detrimentally impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_count_filter(sample_points, target):  \n",
    "    \"\"\"\n",
    "    Returns modified sample_points dataframe. Classes in the target feature that have counts of 1 are removed. \n",
    "    \"\"\"  \n",
    "    # Group by FVT\n",
    "    LF22_FVT_counts = sample_points.groupby(target).count()\n",
    "\n",
    "    # Identify FVTs with low observation counts\n",
    "    low_count_FVT = LF22_FVT_counts[LF22_FVT_counts.iloc[:,0] < 5].index.tolist()\n",
    "\n",
    "    # Remove those FVTs from sample_points\n",
    "    sample_points = sample_points.loc[~sample_points[target].isin(low_count_FVT)]\n",
    "    print(f\"Low count {target}: {low_count_FVT}\")\n",
    "    print(f\"After removing low count {target}: {sample_points.shape}\")\n",
    "    print(f\"Sample Points Attributes: {sample_points.columns}\")\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Prepare the Sample Points__\n",
    "Before training the model, the sample points need to be filtered. Filtering steps are:\n",
    "- Remove near points - defined as points that are less than 70m apart.\n",
    "- Remove Null points (-9999/-1111 values) - these will not be updated in the final raster. \n",
    "- Remove undisturbed values - the model is intended to update points that have been disturbed in the last 10 years. \n",
    "- Remove points with agricultural or developed FVT classes - these are updated using datasets not used for other classes. \n",
    "- Remove observations with classes that have very low representation (less than 5 observations in the class) in the dataset. This is done to allow train_test_split to run. \n",
    "- Optionally, join in additional features. feature_list may be modified to select which features to add in (see join_features for further details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(sample_point_fpath, target):\n",
    "    \"\"\"\n",
    "    Returns dataframe of processed sample points. Sample points are read in from shapefile. Processing includes filtering out null points and developed/agricultural FVTs as \n",
    "    well as appending additional features. \n",
    "    \"\"\"\n",
    "\n",
    "    # Read in gdf\n",
    "    sample_points = gpd.read_file(sample_point_fpath)\n",
    "    print(f\"Total Number of points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Filter out near points\n",
    "    sample_points = sample_points.loc[sample_points['NEAR_FID'] == -1]\n",
    "    print(f\"After removing near points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Drop unneeeded columns if present\n",
    "    sample_points = sample_points.drop(['Classified', 'GrndTruth', 'NEAR_FID', 'NEAR_DIST'], axis=1,\n",
    "                                       errors='ignore')\n",
    "    \n",
    "    # Remove observations with -9999/-1111 in any field \n",
    "    sample_points = sample_points.loc[~sample_points.isin([-1111, -9999]).any(axis=1)]\n",
    "    print(f\"After removing null points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Filter out points that weren't disturbed\n",
    "    sample_points = sample_points.loc[sample_points['LF22_FDST'] != 0]\n",
    "    print(f\"After removing undisturbed points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Join in additional features\n",
    "    sample_points = join_features(sample_points)\n",
    "\n",
    "    # Filter out agricultural and developed points\n",
    "    developed_fvt = list(range(20,33)) + list(range(2901,2906))\n",
    "    ag_fvt = [80, 81, 82] + list(range(2960, 2971))\n",
    "    fvt_filter = developed_fvt + ag_fvt\n",
    "    sample_points = sample_points.loc[~sample_points['LF22_FVT'].isin(fvt_filter)]\n",
    "\n",
    "    # Filter out classes in the target feature with very low counts\n",
    "    sample_points = low_count_filter(sample_points, target)\n",
    "\n",
    "    return sample_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Instantiate Estimators__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Histogram Based Gradient Boosting Classifier__\n",
    "Scikit-learn implementation of Histogram Based Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histGradientBoostingClassifier(categorical_feature_list, class_weight='Balanced', seed=1234):\n",
    "    \"\"\"\n",
    "    Returns specified histogram-based gradient boosting classifier. \n",
    "    \"\"\"\n",
    "\n",
    "    hgb_classifier = HistGradientBoostingClassifier(\n",
    "        categorical_features=categorical_feature_list,  # Natively handle categorical variables\n",
    "        class_weight=class_weight,\n",
    "        random_state=seed,\n",
    "        learning_rate=0.01,\n",
    "        max_iter=100\n",
    "    )\n",
    "\n",
    "    return hgb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Catboost Gradient Boosting__\n",
    "Catboost implementation of Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catBoost_hgbc(cat_features):\n",
    "    \"\"\"\n",
    "    Returns specified gradient boosting classifier from catBoost library. \n",
    "    \"\"\"\n",
    "    cb_hgbc = CatBoostClassifier(\n",
    "        iterations=2000, \n",
    "        learning_rate=0.01,\n",
    "        loss_function=\"MultiClass\",\n",
    "        #boosting_type=\"Plain\",\n",
    "        cat_features=cat_features,  # List of categorical features\n",
    "        auto_class_weights='Balanced', # Used to handle class imbalance\n",
    "        random_seed=1234,\n",
    "        task_type='GPU'  # Train the model on GPU\n",
    "    )\n",
    "\n",
    "    return cb_hgbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Random Forest Classifier__\n",
    "Scikit-learn implementation Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestClassifier(n_est=100, min_samples_leaf=50, bootstrap=True, oob_score=True, \n",
    "                           n_jobs=-1, random_state=1234, max_features='auto', class_weight=None):\n",
    "    \"\"\"\n",
    "    Returns specified Random Forest Classifier. \n",
    "    \"\"\"\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=n_est,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        bootstrap=bootstrap,\n",
    "        oob_score=oob_score,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        max_features=max_features,\n",
    "        class_weight=class_weight,\n",
    "        max_depth = None\n",
    "    )\n",
    "\n",
    "    return rf_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Multi-Layer Perceptron__\n",
    "Scikit-learn implementation of Multi-Layer Perceptron. Note that this has not been tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiLayerPerceptron():\n",
    "    \"\"\"\n",
    "    Returns specified Multi Layer Perceptron.\n",
    "    \"\"\"\n",
    "    clf = MLPClassifier(\n",
    "        max_iter = 10000,\n",
    "        activation = 'relu',  # 'relu' is the default\n",
    "        hidden_layer_sizes = (256, 128, 64)\n",
    "    )\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Model__\n",
    "Trains and returns HGBC model provided data. Subsets the training data to a provided list of predictors and predicts the specified target feature.\n",
    "\n",
    "__Note__: Aspect, Elevation, and Slope are the only continuous LANDFIRE datasets, therefore any feature not in that list is assumed to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, predictors, target, model_type='HGBC'):\n",
    "    \"\"\"\n",
    "    Returns trained model of specified type given a set of predictors. \n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = \"balanced\"\n",
    "\n",
    "    # Get list of predictors for run\n",
    "    cat_features = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]  # Categorical features for HGBC models  \n",
    "    \n",
    "    # Setting the data type to categorical speeds up model training for catBoost\n",
    "    if model_type == \"catBoost\":\n",
    "        train_data[cat_features] = train_data[cat_features].astype(\"category\")\n",
    "\n",
    "    # Separate training data predictors/response\n",
    "    y_train = train_data[target].copy()\n",
    "    X_train = train_data[predictors].copy()\n",
    "\n",
    "    # Fit selected model_type\n",
    "    if model_type == 'RF':\n",
    "        model = randomForestClassifier(n_est=100, min_samples_leaf=50, bootstrap=True, oob_score=True,\n",
    "                                            n_jobs=-1, random_state=1234, max_features='sqrt', class_weight=class_weight)\n",
    "    elif model_type == 'HGBC':\n",
    "        model = histGradientBoostingClassifier(categorical_feature_list=cat_features, class_weight=class_weight)\n",
    "    \n",
    "    elif model_type == 'catBoost':\n",
    "        model = catBoost_hgbc(cat_features=cat_features)\n",
    "\n",
    "    elif model_type == 'MLP':\n",
    "        print('Defining MLP...')\n",
    "        model = multiLayerPerceptron()\n",
    "    \n",
    "    # Fit the model with the training data\n",
    "    print('Fitting Model...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the fit model\n",
    "    return model         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluate Model__\n",
    "Provided a trained model, test data, a set of predictors, and a target - return a list of predictions metrics on prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_data, predictors, target, model_type='HGBC'):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing 1. 'metrics' : a list of metrics quantifying model performance, and 2. 'predictions' : a list of predictions corresponding to \n",
    "    each observation in the sample_points data.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Separate the predictors from target\n",
    "    y_test = test_data[target].copy()\n",
    "    x_test = test_data[predictors].copy()\n",
    "\n",
    "    # Perform prediction\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Get metrics\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    bal_acc = round(balanced_accuracy_score(y_test, y_pred),3)\n",
    "    recall = round(recall_score(y_test, y_pred, average='macro'), 3)\n",
    "    precision = round(precision_score(y_test, y_pred, average='macro'), 3)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='macro'), 3)\n",
    "\n",
    "    print(f\"Predictors: {predictors}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics\" : [accuracy, bal_acc, recall, precision, f1, predictors],\n",
    "        \"predictions\" : y_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Results Evaluation Functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Plot the Class Distribution__\n",
    "Plots the distribution of the variable of interest. Can be set to display the raw counts for each class or the percentage of the dataste that the class accounts for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, group_var, metric='Count', sort=True, title=f'Distribution', figsize=(15, 50)):\n",
    "    \"\"\"\n",
    "    Plots the distribution of group_var. 'metric' can be set to either 'Count' or 'Percent' to change the display type. \n",
    "    The 'sort' argument specifies whether to sort the display in order from most common to least common. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get count of target class\n",
    "    df_gp = df.groupby(group_var).count().reset_index()\n",
    "    df_gp = df_gp.iloc[:, 0:2].rename(columns={df_gp.columns[1] : 'Count'})\n",
    "\n",
    "    # Get classes proportional representation\n",
    "    df_gp['Percent'] = df_gp['Count'] / df_gp['Count'].sum() * 100\n",
    "    df_gp[group_var] = df_gp[group_var].astype('str')  # For plotting change FVT to str\n",
    "    if sort == True: \n",
    "        df_gp = df_gp.sort_values(by=\"Count\", ascending=True)\n",
    "\n",
    "    # Visualize the distribution\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    hbars = ax.barh(df_gp[group_var],\n",
    "                df_gp[metric],\n",
    "                align='center')\n",
    "    ax.set(title=title,\n",
    "        xlabel= f'{metric}', \n",
    "        ylabel=f'{group_var}')  \n",
    "    ax.bar_label(hbars, fmt=\" %.2f\")\n",
    "    \n",
    "    # Remove whitespace at margins\n",
    "    plt.ylim(ax.patches[0].get_y(), ax.patches[-1].get_y() + ax.patches[-1].get_height()+0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Plot Confusion Matrix__\n",
    "Generates a confusion matrix of true labels vs. predicted labels.\n",
    "Normalization options are: \n",
    "- 'true' : Normalizes over the rows.\n",
    "- 'pred' : Normalizes over the columns. \n",
    "- 'all' : Normalizes over the entire population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix\n",
    "def plot_cm(y_true, y_pred, normalize=None, figsize=10, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix of true values (y_true) against predicted values (y_pred). Can be normalized by setting \n",
    "    normalize='true'/'pred'/'all'.\n",
    "    \"\"\"\n",
    "\n",
    "    np.set_printoptions(precision=2) \n",
    "\n",
    "    # Define title\n",
    "    title = f\"{title}, normalize = {normalize}\"\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                                   cmap=plt.cm.Blues,\n",
    "                                                   normalize=normalize,\n",
    "                                                   xticks_rotation='vertical'\n",
    "                                                   )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    # Set plot size\n",
    "    fig = disp.ax_.get_figure()\n",
    "    fig.set_figwidth(figsize)\n",
    "    fig.set_figheight(figsize)\n",
    "\n",
    "    # Display results\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Main__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVT: [2016, 2064, 2066, 2099, 2128, 2177, 2184, 2211, 2219, 2260, 2567, 2910, 2911]\n",
      "After removing low count LF22_FVT: (160346, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n",
      "Fitting Model...\n",
      "Model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\\models\\LF22_FVT_HGBC_model_2024-05-16_11-30-32\n"
     ]
    }
   ],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "\n",
    "# Define predictors and target\n",
    "predictors = ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "# Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "# Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  \n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=sample_points, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Save model \n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "model_fpath = os.path.join(paths_dict['model_dir'], model_fname)\n",
    "dump(model, model_fpath)\n",
    "print(f\"Model written to: {model_fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __FVT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVT: [2016, 2064, 2066, 2099, 2128, 2177, 2184, 2211, 2219, 2260, 2567, 2910, 2911]\n",
      "After removing low count LF22_FVT: (163057, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Model...\n",
      "Score on the Training Set: \n",
      "Predictors: ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
      "Accuracy: 0.733\n",
      "Balanced Accuracy: 0.899\n",
      "Recall: 0.899\n",
      "Precision: 0.68\n",
      "F1: 0.756\n",
      "Score on the test set: \n",
      "Predictors: ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
      "Accuracy: 0.67\n",
      "Balanced Accuracy: 0.558\n",
      "Recall: 0.558\n",
      "Precision: 0.462\n",
      "F1: 0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcco573\\AppData\\Local\\miniconda3\\envs\\geospatial_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, features=predictors)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test set\n",
    "test[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=f'{target}_model_results')\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test set using the FVT predictions. This will then be fed into the FVC/FVH models to see how replacing the true FVT values with predicted values will impact the FVC/FVH predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvt_test = test.copy()\n",
    "fvt_test[\"LF22_FVT_Predictions\"] = results['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Train model to Create Predicted FVT Raster__\n",
    "Use all sample points to train model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVT: [2016, 2064, 2066, 2099, 2128, 2177, 2184, 2211, 2219, 2260, 2567, 2910, 2911]\n",
      "After removing low count LF22_FVT: (160346, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n",
      "Fitting Model...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_fname'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m train_model(train_data\u001b[38;5;241m=\u001b[39msample_points, predictors\u001b[38;5;241m=\u001b[39mpredictors, target\u001b[38;5;241m=\u001b[39mtarget, model_type\u001b[38;5;241m=\u001b[39mmodel_type)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Save model \u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m model_fpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(paths_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mpaths_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_fname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     22\u001b[0m dump(model, model_fpath)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel written to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_fpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model_fname'"
     ]
    }
   ],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "\n",
    "# Define predictors and target\n",
    "predictors = ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "# Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "# Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  \n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=sample_points, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Save model \n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "model_fpath = os.path.join(paths_dict['model_dir'], model_fname)\n",
    "dump(model, model_fpath)\n",
    "print(f\"Model written to: {model_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\\models\\LF22_FVT_HGBC_model_2024-05-10_16-49-07\n"
     ]
    }
   ],
   "source": [
    "# Save model \n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "model_fpath = os.path.join(paths_dict['model_dir'], model_fname)\n",
    "dump(model, model_fpath)\n",
    "print(f\"Model written to: {model_fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __FVC__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVC: [117]\n",
      "After removing low count LF22_FVC: (163088, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n",
      "      LF22_FVT  LF20_FVT  LF16_FVT  LF16_EVT  LF14_EVT  LF22_FDST  ZONE   BPS  \\\n",
      "7742      2126      2146      2274      9309      3145        113    12  1069   \n",
      "7743      2146      2146      2146      7146      3145        113    12  1069   \n",
      "7744      2126      2126      2126      7126      3019        113    12  1050   \n",
      "7745      2126      2126      2126      7126      3220        113    12  1069   \n",
      "7746      2272      2126      2126      7126      3011        113    12  1048   \n",
      "\n",
      "      ASPECT  SLOPE  ...  BPS_FRG_NE                          geometry  \\\n",
      "7742     203      5  ...           8  POINT (-1817051.280 2140842.605)   \n",
      "7743     125      7  ...           8  POINT (-1816871.280 2140842.605)   \n",
      "7744     297     22  ...           5  POINT (-1817411.280 2140032.605)   \n",
      "7745     128      5  ...           8  POINT (-1817231.280 2139822.605)   \n",
      "7746      51     11  ...           2  POINT (-1817081.280 2139642.605)   \n",
      "\n",
      "                                            BPS_NAME  BPS_FRG  FDST_TYPE  \\\n",
      "7742  Inter-Mountain Basins Montane Sagebrush Steppe     IV-A       Fire   \n",
      "7743  Inter-Mountain Basins Montane Sagebrush Steppe     IV-A       Fire   \n",
      "7744             Great Basin Pinyon-Juniper Woodland      V-A       Fire   \n",
      "7745  Inter-Mountain Basins Montane Sagebrush Steppe     IV-A       Fire   \n",
      "7746        Rocky Mountain Aspen Forest and Woodland      I-C       Fire   \n",
      "\n",
      "      FDST_SEV          FDST_TSD           EVT_PHYS  \\\n",
      "7742       Low  Six to Ten Years  Exotic Herbaceous   \n",
      "7743       Low  Six to Ten Years          Grassland   \n",
      "7744       Low  Six to Ten Years          Shrubland   \n",
      "7745       Low  Six to Ten Years          Shrubland   \n",
      "7746       Low  Six to Ten Years          Shrubland   \n",
      "\n",
      "                                         EVT_GP_N               EVT_CLASS  \n",
      "7742  Introduced Perennial Grassland and Forbland  Herbaceous - grassland  \n",
      "7743                                    Grassland  Herbaceous - grassland  \n",
      "7744           Big Sagebrush Shrubland and Steppe               Shrubland  \n",
      "7745           Big Sagebrush Shrubland and Steppe               Shrubland  \n",
      "7746           Big Sagebrush Shrubland and Steppe               Shrubland  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVC'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Model...\n",
      "Score on the Training Set: \n",
      "Predictors: ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
      "Accuracy: 0.991\n",
      "Balanced Accuracy: 0.996\n",
      "Recall: 0.996\n",
      "Precision: 0.966\n",
      "F1: 0.976\n",
      "Score on the test set: \n",
      "Predictors: ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
      "Accuracy: 0.991\n",
      "Balanced Accuracy: 0.988\n",
      "Recall: 0.988\n",
      "Precision: 0.96\n",
      "F1: 0.965\n"
     ]
    }
   ],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'new_dir_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m metrics_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [model_type]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m## Save out the dataframes\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m out_dir \u001b[38;5;241m=\u001b[39m make_dir(paths_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_base_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], new_dir_name\u001b[38;5;241m=\u001b[39m\u001b[43mpaths_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnew_dir_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m preds_out_fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m test_points\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, preds_out_fname))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'new_dir_name'"
     ]
    }
   ],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test_points set\n",
    "test_points[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=paths_dict['new_dir_name'])\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test_points.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVC: [117]\n",
      "After removing low count LF22_FVC: (163088, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n",
      "Fitting Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\mcco573\\\\OneDrive - PNNL\\\\Documents\\\\_Projects\\\\BPA Wildfire\\\\Fuel Attributes Model\\\\models\\\\LF22_FVC_HGBC_model_2024-05-15_15-53-00']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a model using the full dataset\n",
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVC'\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=sample_points, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(paths_dict['model_dir'], model_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on multiple sets of predictors\n",
    "Used to compare/contrast the performance of the model using various sets of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple sets of predictors to compare\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors - includes target and predictors\n",
    "predictors_list = [\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE'],\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF16_FVC', 'LF22_FDST', 'ZONE'],\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF16_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "    ['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF20_FVC', 'LF16_FVC', 'LF20_FVH', 'LF16_FVH', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME']\n",
    "]\n",
    "\n",
    "# Define results DF\n",
    "results_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'predictors', 'model_type', 'train/test'])\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "for predictors in predictors_list: \n",
    "    # Train model\n",
    "    model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "    # Evaluate Model\n",
    "    print(\"Score on the Training Set: \")\n",
    "    results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "    results_df.loc[len(results_df)] = results['metrics'] + [f'{model_type}', 'train']\n",
    "\n",
    "    print(\"Score on the test set: \")\n",
    "    results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)\n",
    "    results_df.loc[len(results_df)] = results['metrics'] + [f'{model_type}', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __FVC with Predicted FVT__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVC'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath = os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and run model\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "## Define set of predictors - includes target and predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE']\n",
    "\n",
    "## Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "## Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "## Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on test set with predicted FVTs\")\n",
    "results = eval_model(model=model, test_data=fvt_test, predictors=predictors, target=target, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test_points set\n",
    "test_points[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=paths_dict['new_dir_name'])\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test_points.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  __FVH__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVH: []\n",
      "After removing low count LF22_FVH: (163092, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVH'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath = os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Model...\n",
      "Score on the Training Set: \n",
      "Predictors: ['LF22_FVT', 'LF20_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
      "Accuracy: 0.993\n",
      "Balanced Accuracy: 0.994\n",
      "Recall: 0.994\n",
      "Precision: 0.953\n",
      "F1: 0.966\n",
      "Score on the test set: \n",
      "Predictors: ['LF22_FVT', 'LF20_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
      "Accuracy: 0.992\n",
      "Balanced Accuracy: 0.991\n",
      "Recall: 0.991\n",
      "Precision: 0.941\n",
      "F1: 0.954\n"
     ]
    }
   ],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors - includes target and predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "#predictors = ['LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH',  target]\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'new_dir_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m metrics_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [model_type]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m## Save out the dataframes\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m out_dir \u001b[38;5;241m=\u001b[39m make_dir(paths_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_base_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], new_dir_name\u001b[38;5;241m=\u001b[39m\u001b[43mpaths_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnew_dir_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m preds_out_fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m test_points\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, preds_out_fname))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'new_dir_name'"
     ]
    }
   ],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test_points set\n",
    "test_points[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=paths_dict['new_dir_name'])\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test_points.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVH: []\n",
      "After removing low count LF22_FVH: (163092, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n",
      "Fitting Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\mcco573\\\\OneDrive - PNNL\\\\Documents\\\\_Projects\\\\BPA Wildfire\\\\Fuel Attributes Model\\\\models\\\\LF22_FVH_HGBC_model_2024-05-15_15-57-07']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a model using the full dataset\n",
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVH'\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=sample_points, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(paths_dict['model_dir'], model_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a list of models\n",
    "Run multiple sets of predictors to evaluate against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC\n",
    "\n",
    "# Generate combinations of potential predictor sets\n",
    "##predictors = ['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'EVT_GP_N', 'EVT_PHYS', 'EVT_CLASS']\n",
    "##predictors_list = generate_combinations(predictors, 'LF22_FVT')\n",
    "\n",
    "# Alternatively, get list of lists of predictors sets from file \n",
    "##predictors_list = get_runs()\n",
    "\n",
    "# Alternatively, define a list of lists of attribute combinations\n",
    "predictors_list = [\n",
    "    ['ASPECT', 'ELEVATION', 'SLOPE', 'LF22_FDST', 'ZONE'],\n",
    "    ['ASPECT', 'ELEVATION', 'SLOPE', 'LF22_FDST', 'ZONE'],\n",
    "    ['ASPECT', 'ELEVATION', 'SLOPE', 'LF20_EVT', 'LF22_FDST', 'ZONE'],\n",
    "    ['LF20_EVT', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "result_df = pd.DataFrame(columns=['Accuracy', 'Balanced_Accuracy', 'Recall', 'Precision', 'F1', 'Predictors'])\n",
    "\n",
    "# Iterate through combinations of predictors\n",
    "for predictors in predictors_list:\n",
    "    # Train model\n",
    "    model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "    # Evaluate Model\n",
    "    results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)['metrics']\n",
    "\n",
    "    result_df.loc[len(result_df)] = results\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "----\n",
    "\n",
    "!!! __NOTE:__ THIS HAS NOT BEEN UPDATED TO REFLECT CHANGES TO FUNCTOINS\n",
    "\n",
    "\n",
    "As of 4/23/24 the best performance achieved has been an accuracy of ~60% using a HGBC and the predictors: ['LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF22_FVT']. \n",
    "\n",
    "All parameters were set to default except for: \n",
    "- categorical_features = List of categorical features\n",
    "- class_weight = \"balanced\"\n",
    "- random_state = 1234\n",
    "- learning_rate = .01\n",
    "- max_iter = 50\n",
    "\n",
    "After tinkering with a number of hyperparameters, but not conducting a full grid search, it appears that tuning the parameters may not yield significant improvements. \n",
    "\n",
    "__Goal__: Try to tease out what is causing usch low model performance. Is it due to under/over fitting, or is the set of predictors insufficent to further separate out the classes? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model \n",
    "seed = 1234\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "# Define train/test split\n",
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "\n",
    "# Define path to sample points\n",
    "sample_points_fpath = paths_dict['sample_point_fpath']\n",
    "\n",
    "# Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath)\n",
    "\n",
    "# Perform Train/Test Split\n",
    "train, test = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])\n",
    "\n",
    "# Standardize the data\n",
    "train, test = standardize_topo(train, test)\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC\n",
    "\n",
    "# Define set of attributes - includes target and predictors\n",
    "attributes = ['LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_SCla', target]\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, attributes=attributes, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "results = eval_model(model=model, test_data=test, attributes=attributes, model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare distributions of actual vs. predicted\n",
    "In general we see a trend where the model underpredicts the most common classes and overpredicts the less common classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric to display - Percent of total or Count\n",
    "metric = 'count'  # 'count' / 'perc'\n",
    "\n",
    "# Create dataframe of predictions vs. true labels\n",
    "pred_df = pd.DataFrame(data={'True' : test['LF22_FVT'], 'Prediction' : results['predictions']})\n",
    "\n",
    "# Get counts of predictions\n",
    "pred_gp = pred_df.groupby('Prediction').count().reset_index()\n",
    "pred_gp = pred_gp[['Prediction', 'True']].rename(columns={'True':'pred_count'})\n",
    "pred_gp['pred_perc'] = pred_gp['pred_count'] / pred_gp['pred_count'].sum() * 100\n",
    "pred_gp['Prediction'] = pred_gp['Prediction'].astype('str')\n",
    "\n",
    "# Get counts of true\n",
    "true_gp = pred_df.groupby('True').count().reset_index()\n",
    "true_gp = true_gp[['True', 'Prediction']].rename(columns={'Prediction':'true_count'})\n",
    "true_gp['true_perc'] = true_gp['true_count'] / true_gp['true_count'].sum() * 100\n",
    "true_gp['True'] = true_gp['True'].astype('str')\n",
    "\n",
    "# Join the dataframes\n",
    "pred_gp = pred_gp.set_index('Prediction')\n",
    "true_gp = true_gp.set_index('True')\n",
    "\n",
    "fvt_gp = true_gp.join(pred_gp, how='inner')\n",
    "fvt_gp = fvt_gp.sort_values(by='true_count')\n",
    "\n",
    "# Plot the data side-by-side\n",
    "ind = np.arange(len(fvt_gp))  # The y locations for the groups\n",
    "width = 0.3  # The width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,40))\n",
    "true_bars = ax.barh(ind+width/2, fvt_gp[f'true_{metric}'], width, label='Actual')\n",
    "pred_bars = ax.barh(ind-width/2, fvt_gp[f'pred_{metric}'], width, label='Predicted')\n",
    "\n",
    "ax.set_title('Actual vs. Predicted FVT Distribution')\n",
    "ax.set_ylabel('FVT')\n",
    "ax.set_xlabel(f'{metric}')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(fvt_gp.index.values)\n",
    "ax.legend()\n",
    "ax.bar_label(true_bars, fmt=\" %.2f\")\n",
    "ax.bar_label(pred_bars, fmt=\" %.2f\")\n",
    "\n",
    "plt.ylim(ax.patches[0].get_y()-0.5, ax.patches[-1].get_y() + ax.patches[-1].get_height()+0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much does each class contribute to the total classification error?\n",
    "Where is the classification error coming from? The top most frequently misclassified, 2045 and 2080, contribute close to 8% of the error, and the rest is distributed across the other classes. These two classes also happen to be the most dominant on the landscape. Although it's interesting that the classifier performed better on 2028, despite it be almost as prevalent as 2045.\n",
    "\n",
    "- 2045: Tr Northern Rocky Mountain Dry-Mesic Montane Mixed Conifer Forest\n",
    "- 2080: Sh Inter-Mountain Basins Big Sagebrush Shrubland\n",
    "- 2028: Tr Mediterranean California Mesic Mixed Conifer Forest and Woodland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric to display - Count or Percent\n",
    "metric = 'perc_total_misclass'  # 'count' / 'perc' / 'perc_total_misclass\n",
    "\n",
    "# Look at misclass distributions\n",
    "fvt_gp['diff_count'] = fvt_gp['true_count'] - fvt_gp['pred_count']  # Get the count of misclasses\n",
    "fvt_gp['diff_perc'] = abs(fvt_gp['diff_count']) / abs(fvt_gp['diff_count']).sum() * 100  # Get the percentage of total misclasses\n",
    "fvt_gp['diff_perc_total_misclass']  = fvt_gp['diff_perc'] * (1 - results['metrics'][0])  # Get the percent contribution to misclassification\n",
    "fvt_gp_diff_sort = fvt_gp.sort_values(by='diff_perc')\n",
    "\n",
    "# Plot the data side-by-side\n",
    "fig, ax = plt.subplots(figsize=(15,40))\n",
    "diff_bars = ax.barh(fvt_gp_diff_sort.index.values, fvt_gp_diff_sort[f'diff_{metric}'])\n",
    "\n",
    "\n",
    "ax.set_ylabel('FVT')\n",
    "ax.set_xlabel(f'{metric}')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(fvt_gp_diff_sort.index.values)\n",
    "ax.bar_label(diff_bars, fmt=\" %.2f\")\n",
    "\n",
    "plt.ylim(ax.patches[0].get_y()-0.5, ax.patches[-1].get_y() + ax.patches[-1].get_height()+0.5)\n",
    "\n",
    "print(f\"Total Misclass Error: {round((1-results['metrics'][0]) * 100, 2)}%\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the confusion matrix\n",
    "Looking at the two most commonly misclassified classes (2080, 2045), how are they being misclassified? \n",
    "\n",
    "__2045__: Tr Northern Rocky Mountain Dry-Mesic Montane Mixed Conifer Forest\n",
    "\n",
    "60% of samples were classified correctly. False predictions were fairly spread over the classes, but the most common mispredictions were:\n",
    "- 2047 (10%): Tr Northern Rocky Mountain Mesic Montane Mixed Conifer Forest\n",
    "- 2053 (7.6%): Tr Northern Rocky Mountain Ponderosa Pine Woodland and Savanna\n",
    "- 2227 (6.3%): Tr Pseudotsuga menziesii Forest Alliance\n",
    "\n",
    "__2080__: Sh Inter-Mountain Basins Big Sagebrush Shrubland\n",
    "\n",
    "58% of samples were correctly classifed. False predictions were also fairly spread over the classes, but the most common mispredictoins were:\n",
    "- 2125 (9.8%): Sh Inter-Mountain Basins Big Sagebrush Steppe\n",
    "- 2065 (6.1%): Sh Columbia Plateau Scabland Shrubland\n",
    "- 2123 (5.1%): He Columbia Plateau Steppe and Grassland\n",
    "- 2282 (4.4%): Sh Great Basin & Intermountain Ruderal Shrubland\n",
    "- 2273 (3.8%): He Great Basin & Intermountain Introduced Annual Grassland\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize where misclasses are occurring using confusion matrix\n",
    "plot_cm(y_true = test[target], y_pred=results[\"predictions\"], normalize='true', figsize=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the class-specific misclassification distributions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_ref_dicts = read_ref_data()\n",
    "\n",
    "pred_df['True_FVT_NAME'] = pred_df['True'].map(LF_ref_dicts['LF22_FVT']) \n",
    "pred_df['Pred_FVT_NAME'] = pred_df['Prediction'].map(LF_ref_dicts['LF22_FVT']) \n",
    "\n",
    "pred_df['True_EVT_CLASS'] = pred_df['True'].map(LF_ref_dicts['FVT_CLASS']) \n",
    "pred_df['Pred_EVT_CLASS'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_CLASS']) \n",
    "\n",
    "pred_df['True_EVT_PHYS'] = pred_df['True'].map(LF_ref_dicts['FVT_PHYS']) \n",
    "pred_df['Pred_EVT_PHYS'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_PHYS']) \n",
    "\n",
    "pred_df['True_GP_N'] = pred_df['True'].map(LF_ref_dicts['FVT_GP_N']) \n",
    "pred_df['Pred_GP_N'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_GP_N']) \n",
    "\n",
    "pred_df['True_LF'] = pred_df['True'].map(LF_ref_dicts['FVT_LF']) \n",
    "pred_df['Pred_LF'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_LF']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2045__: Tr Northern Rocky Mountain Dry-Mesic Montane Mixed Conifer Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_2045 = pred_df.loc[pred_df['True'] == 2045]\n",
    "class_distribution(true_2045, metric = 'Percent', variable=\"Pred_FVT_NAME\", group_var=\"True\", sort='False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2080__: Sh Inter-Mountain Basins Big Sagebrush Shrubland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_2080 = pred_df.loc[pred_df['True'] == 2080]\n",
    "class_distribution(true_2080, metric = 'Percent', variable=\"Pred_FVT_NAME\", group_var=\"True\", sort='False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the misclassifications ecologically similar?\n",
    "-----\n",
    "If the misclassifications are ecologically similar then they may not effect the F40 classification.\n",
    "\n",
    "__QUESTIONS__: \n",
    "1. Are the results ecologically similar enough to use with the F40 model?\n",
    "2. Is there a pattern that suggests what the model is struggling to separate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions and join the EVT_* groupings\n",
    "pred_df = pd.DataFrame(data={'True' : test['LF22_FVT'], 'Prediction' : results['predictions']})\n",
    "\n",
    "LF_ref_dicts = read_ref_data()\n",
    "\n",
    "pred_df['True_FVT_NAME'] = pred_df['True'].map(LF_ref_dicts['LF22_FVT']) \n",
    "pred_df['Pred_FVT_NAME'] = pred_df['Prediction'].map(LF_ref_dicts['LF22_FVT']) \n",
    "\n",
    "pred_df['True_EVT_FUEL'] = pred_df['True'].map(LF_ref_dicts['FVT_EVT'])\n",
    "pred_df['Pred_EVT_FUEL'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_EVT'])\n",
    "\n",
    "pred_df['True_EVT_CLASS'] = pred_df['True'].map(LF_ref_dicts['FVT_CLASS']) \n",
    "pred_df['Pred_EVT_CLASS'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_CLASS']) \n",
    "\n",
    "pred_df['True_EVT_PHYS'] = pred_df['True'].map(LF_ref_dicts['FVT_PHYS']) \n",
    "pred_df['Pred_EVT_PHYS'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_PHYS']) \n",
    "\n",
    "pred_df['True_GP_N'] = pred_df['True'].map(LF_ref_dicts['FVT_GP_N']) \n",
    "pred_df['Pred_GP_N'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_GP_N']) \n",
    "\n",
    "pred_df['True_LF'] = pred_df['True'].map(LF_ref_dicts['FVT_LF']) \n",
    "pred_df['Pred_LF'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_LF']) \n",
    "\n",
    "# Remove the NaNs - the confusion matrix won't work with them\n",
    "null_count = len(pred_df.loc[pred_df.isnull().any(axis=1)])\n",
    "total_count = len(pred_df)\n",
    "print(f\"Removed {null_count} null values out of {total_count} points ({round(null_count/total_count * 100, 2)}%).\")\n",
    "df = pred_df.loc[~pred_df.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Desktop\\temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points.to_csv(r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Desktop\\temp_sample_pts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy of LF assignment\n",
    "accuracy = accuracy_score(df['True_LF'], df['Pred_LF']) * 100\n",
    "print(f\"Accuracy: {round(accuracy, 2)}%\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_cm(df['True_LF'], df['Pred_LF'], figsize=10, normalize=None, title=\"EVT_LF Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EVT_CLASS Grouping\n",
    "\n",
    "# Get the accuracy of CLASS assignment\n",
    "accuracy = accuracy_score(df['True_EVT_CLASS'], df['Pred_EVT_CLASS']) * 100\n",
    "print(f\"Accuracy: {round(accuracy, 2)}%\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_cm(df['True_EVT_CLASS'], df['Pred_EVT_CLASS'], figsize=10, normalize='true', title=\"EVT_CLASS Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EVT_PHYS Groupings\n",
    "\n",
    "# Get the accuracy of PHYS assignment\n",
    "accuracy = accuracy_score(df['True_EVT_PHYS'], df['Pred_EVT_PHYS']) * 100\n",
    "print(f\"Accuracy: {round(accuracy, 2)}%\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_cm(df['True_EVT_PHYS'], df['Pred_EVT_PHYS'], figsize=15, normalize='true', title='EVT_PHYS Groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EVT_GP_N Groupings\n",
    "\n",
    "# Get the accuracy of PHYS assignment\n",
    "accuracy = accuracy_score(df['True_GP_N'], df['Pred_GP_N']) * 100\n",
    "print(f\"Accuracy: {round(accuracy, 2)}%\")\n",
    "\n",
    "plot_cm(df['True_GP_N'], df['Pred_GP_N'], figsize=60, normalize='true', title=\"EVT_GP_N Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the effect of minority classes\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate FVT Class Distribution\n",
    "Before digging into where misclasses are occurring, look at the distribution of FVT classes. At first inspection there are many classes with very few occurrences. The increase in cardinality that results from including these classes may significantly degrade model performance by obscuring signal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overall class distribution of sample points\n",
    "class_distribution(sample_points, metric='Percent', variable='LF22_FVT', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to sample points\n",
    "sample_points_fpath = paths_dict['sample_point_fpath']\n",
    "\n",
    "# Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath)\n",
    "\n",
    "print(f\"\\n\")\n",
    "## Get counts of target class\n",
    "variable = 'LF22_FVT'\n",
    "df_fvt_gp = sample_points.groupby(variable).count().reset_index()\n",
    "df_fvt_gp = df_fvt_gp[[variable, 'LF20_FVT']].rename(columns={'LF20_FVT':'Count'})\n",
    "df_fvt_gp['Percent'] = df_fvt_gp['Count'] / df_fvt_gp['Count'].sum() * 100\n",
    "df_fvt_gp[variable] = df_fvt_gp[variable].astype('str')  # For plotting change FVT to str\n",
    "df_fvt_gp = df_fvt_gp.sort_values(by='Count', ascending=False)\n",
    "\n",
    "## How many classes comprise 90% of the data?\n",
    "cumsum = 0\n",
    "majority_classes = []\n",
    "\n",
    "# Get list of the most common classes that comprise 90% of the data\n",
    "for i in range(len(df_fvt_gp)):\n",
    "    cumsum += df_fvt_gp.iloc[i]['Percent']\n",
    "    majority_classes.append(int(df_fvt_gp.iloc[i]['LF22_FVT']))\n",
    "\n",
    "    if cumsum >= 90:\n",
    "        break\n",
    "\n",
    "print(f\"Cumulative Percent: {cumsum}\")\n",
    "print(f\"{len(majority_classes)} Majority Classes out of {len(df_fvt_gp.index)} Total:\\n{majority_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Model Performance after dropping the classes comprising the bottom 10% of the data\n",
    "We may be able to improve model performance by dropping the 166 classes that comprise the bottom 10% of the data. In doing so, we sacrifice being able to identify those classes in order to correctly classify the most common classes. \n",
    "\n",
    "Dropping those classes improved classification accuracy by only about 8%, but balanced accuracy increased by closer to 20%. I suspect the set of predictors I have are insufficient to adequately separate the classes apart - some piece of information is missing. This likely has to do with the state and transition models used to model sucession. This could be verified by checking where misclassifications are occurring. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model \n",
    "seed = 1234\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "# Define train/test split\n",
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "\n",
    "# Keep only the majority classes identified above\n",
    "sample_points = sample_points.loc[sample_points['LF22_FVT'].isin(majority_classes)]\n",
    "print(f\"After removing all minority classes: {len(sample_points)}\")\n",
    "\n",
    "# Perform Train/Test Split\n",
    "train, test = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])\n",
    "\n",
    "# Standardize the data\n",
    "train, test = standardize_topo(train, test)\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC\n",
    "\n",
    "# Define set of attributes - includes target and predictors\n",
    "attributes = ['LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', target]\n",
    "#attributes = ['LF16_FVT', 'LF20_FVT', 'LF14_EVT', 'ZONE', 'LF22_FDST', 'BPS_NAME', 'BPS_FRG', target]\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, attributes=attributes, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "results = eval_model(model=model, test_data=test, attributes=attributes, model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare distributions of actual vs. predicted (Majority Class Only)\n",
    "In general we see a trend where the model underpredicts the most common classes and overpredicts the less common classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric to display - Percent of total or Count\n",
    "metric = 'count'  # 'count' / 'perc'\n",
    "\n",
    "# Create dataframe of predictions vs. true labels\n",
    "pred_df = pd.DataFrame(data={'True' : test['LF22_FVT'], 'Prediction' : results['predictions']})\n",
    "\n",
    "# Get counts of predictions\n",
    "pred_gp = pred_df.groupby('Prediction').count().reset_index()\n",
    "pred_gp = pred_gp[['Prediction', 'True']].rename(columns={'True':'pred_count'})\n",
    "pred_gp['pred_perc'] = pred_gp['pred_count'] / pred_gp['pred_count'].sum() * 100\n",
    "pred_gp['Prediction'] = pred_gp['Prediction'].astype('str')\n",
    "\n",
    "# Get counts of true\n",
    "true_gp = pred_df.groupby('True').count().reset_index()\n",
    "true_gp = true_gp[['True', 'Prediction']].rename(columns={'Prediction':'true_count'})\n",
    "true_gp['true_perc'] = true_gp['true_count'] / true_gp['true_count'].sum() * 100\n",
    "true_gp['True'] = true_gp['True'].astype('str')\n",
    "\n",
    "# Join the dataframes\n",
    "pred_gp = pred_gp.set_index('Prediction')\n",
    "true_gp = true_gp.set_index('True')\n",
    "\n",
    "fvt_gp = true_gp.join(pred_gp, how='inner')\n",
    "fvt_gp = fvt_gp.sort_values(by='true_count')\n",
    "\n",
    "# Plot the data side-by-side\n",
    "ind = np.arange(len(fvt_gp))  # The y locations for the groups\n",
    "width = 0.3  # The width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,40))\n",
    "true_bars = ax.barh(ind+width/2, fvt_gp[f'true_{metric}'], width, label='Actual')\n",
    "pred_bars = ax.barh(ind-width/2, fvt_gp[f'pred_{metric}'], width, label='Predicted')\n",
    "\n",
    "ax.set_title('Actual vs. Predicted FVT Distribution')\n",
    "ax.set_ylabel('FVT')\n",
    "ax.set_xlabel(f'{metric}')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(fvt_gp.index.values)\n",
    "ax.legend()\n",
    "ax.bar_label(true_bars, fmt=\" %.2f\")\n",
    "ax.bar_label(pred_bars, fmt=\" %.2f\")\n",
    "\n",
    "plt.ylim(ax.patches[0].get_y()-0.5, ax.patches[-1].get_y() + ax.patches[-1].get_height()+0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the confusion matrix (Majority Class Only)\n",
    "Looking at the two most commonly misclassified classes (2080, 2045), how are they being misclassified? \n",
    "\n",
    "__2045__: Tr Northern Rocky Mountain Dry-Mesic Montane Mixed Conifer Forest\n",
    "\n",
    "60% of samples were classified correctly. False predictions were fairly spread over the classes, but the most common mispredictions were:\n",
    "- 2047 (10%): Tr Northern Rocky Mountain Mesic Montane Mixed Conifer Forest\n",
    "- 2053 (7.6%): Tr Northern Rocky Mountain Ponderosa Pine Woodland and Savanna\n",
    "- 2227 (6.3%): Tr Pseudotsuga menziesii Forest Alliance\n",
    "\n",
    "__2080__: Sh Inter-Mountain Basins Big Sagebrush Shrubland\n",
    "\n",
    "58% of samples were correctly classifed. False predictions were also fairly spread over the classes, but the most common mispredictoins were:\n",
    "- 2125 (9.8%): Sh Inter-Mountain Basins Big Sagebrush Steppe\n",
    "- 2065 (6.1%): Sh Columbia Plateau Scabland Shrubland\n",
    "- 2123 (5.1%): He Columbia Plateau Steppe and Grassland\n",
    "- 2282 (4.4%): Sh Great Basin & Intermountain Ruderal Shrubland\n",
    "- 2273 (3.8%): He Great Basin & Intermountain Introduced Annual Grassland\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize where misclasses are occurring using confusion matrix\n",
    "plot_cm(y_true = test[target], y_pred=results[\"predictions\"], normalize='true', figsize=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the class-specific misclassification distributions? (Majority Class Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_ref_dicts = read_ref_data()\n",
    "\n",
    "pred_df['True_FVT_NAME'] = pred_df['True'].map(LF_ref_dicts['LF22_FVT']) \n",
    "pred_df['Pred_FVT_NAME'] = pred_df['Prediction'].map(LF_ref_dicts['LF22_FVT']) \n",
    "\n",
    "pred_df['True_EVT_CLASS'] = pred_df['True'].map(LF_ref_dicts['FVT_CLASS']) \n",
    "pred_df['Pred_EVT_CLASS'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_CLASS']) \n",
    "\n",
    "pred_df['True_EVT_PHYS'] = pred_df['True'].map(LF_ref_dicts['FVT_PHYS']) \n",
    "pred_df['Pred_EVT_PHYS'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_PHYS']) \n",
    "\n",
    "pred_df['True_GP_N'] = pred_df['True'].map(LF_ref_dicts['FVT_GP_N']) \n",
    "pred_df['Pred_GP_N'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_GP_N']) \n",
    "\n",
    "pred_df['True_LF'] = pred_df['True'].map(LF_ref_dicts['FVT_LF']) \n",
    "pred_df['Pred_LF'] = pred_df['Prediction'].map(LF_ref_dicts['FVT_LF']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_2045 = pred_df.loc[pred_df['True'] == 2045]\n",
    "class_distribution(true_2045, metric = 'Percent', variable=\"Pred_FVT_NAME\", group_var=\"True\", sort='False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2080__: Sh Inter-Mountain Basins Big Sagebrush Shrubland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_2080 = pred_df.loc[pred_df['True'] == 2080]\n",
    "class_distribution(true_2080, metric = 'Percent', variable=\"Pred_FVT_NAME\", group_var=\"True\", sort='False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the misclassifications ecologically similar? (Majority Class Only)\n",
    "\n",
    "If the misclassifications are ecologically similar then they may not effect the F40 classification.\n",
    "\n",
    "__QUESTIONS__: \n",
    "1. Are the results ecologically similar enough to use with the F40 model?\n",
    "2. Is there a pattern that suggests what the model is struggling to separate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EVT_PHYS Groupings\n",
    "\n",
    "# Get the accuracy of PHYS assignment\n",
    "accuracy = accuracy_score(df['True_EVT_PHYS'], df['Pred_EVT_PHYS']) * 100\n",
    "print(f\"Accuracy: {round(accuracy, 2)}%\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_cm(df['True_EVT_PHYS'], df['Pred_EVT_PHYS'], figsize=10, normalize='true', title='EVT_PHYS Groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EVT_GP_N Groupings\n",
    "\n",
    "# Get the accuracy of PHYS assignment\n",
    "accuracy = accuracy_score(df['True_GP_N'], df['Pred_GP_N']) * 100\n",
    "print(f\"Accuracy: {round(accuracy, 2)}%\")\n",
    "\n",
    "plot_cm(df['True_GP_N'], df['Pred_GP_N'], figsize=40, normalize='true', title=\"EVT_GP_N Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
