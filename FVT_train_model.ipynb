{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "\n",
    "__Author__: Pat McCornack\n",
    "\n",
    "__Date__: 4/9/24\n",
    "\n",
    "__Purpose__: Train model to update FVT using most recent FDist data. \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from joblib import dump\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Paths\n",
    "Define set of filepaths to conveniently switch between working off local files or the PNNL drive. Set active_data_dir to either local_data_dir or pnnl_data_dir depending on which you're working off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_root_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\"\n",
    "pnnl_root_dir = r\"\\\\pnl\\projects\\BPAWildfire\\data\\Landfire\\fuels_modeling\\Fuel Attributes Model\"\n",
    "\n",
    "# Define which data directory to work off of\n",
    "active_root_dir = local_root_dir\n",
    "\n",
    "paths_dict = {\n",
    "    \"out_base_dir\" : os.path.join(active_root_dir, r\"model_outputs\\tabular\"),  # Where to save outputs \n",
    "    \"ref_data_dir\" : os.path.join(active_root_dir, r\"..\\LF_raster_data\\_tables\"),  # Location of LF csvs (e.g. LF22_FVT_230.csv)\n",
    "    \"sample_points_dir\" : os.path.join(active_root_dir, r\"data\\sample_points\"),  # Location of shapefile to train model on\n",
    "    \"sample_points_fname\" : \"sample_points_4-17-24_200k_Disturbed.shp\",  # Name of shapefile to train model on\n",
    "    \"runs_dict_fpath\" : os.path.join(active_root_dir, r\"data\\runs_dict.csv\"),  # Used to test different sets of predictors \n",
    "    \"model_dir\" : os.path.join(active_root_dir, \"models\")\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Functions__\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Create a Directory to Output Modeling Results__\n",
    "Names the output directory using the datetime that the script was run. \n",
    "Returns the name of the directory. The returned directory is used to output the trained model and/or results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(base_dir, new_dir_name='model_results'):\n",
    "        \"\"\"\n",
    "        Returns path to a directory created at the specified base_dir location. \n",
    "\n",
    "        The name of the created directory can optionally be specified using the dir_name argument. \n",
    "        \"\"\"\n",
    "\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        output_dir = os.path.join(base_dir, new_dir_name + \"_\" + datetime)\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Pre-Process the Data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Create Data Dictionaries to Append Features__\n",
    "Some features are separate attributes of the LANDFIRE dataset (e.g. BPS Fire Regime) and others are useful for results analysis (e.g. FDst attributes). These can be mapped to points using LANDFIRE CSVs. The below creates dictionaries to perform that mapping. \n",
    "\n",
    "This function is called by join_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref_data(ref_data_dir=paths_dict[\"ref_data_dir\"]):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of dictionaries of mappings between LANDFIRE raster values and other attributes associated with those values. \n",
    "    \"\"\"\n",
    "    data_dir = ref_data_dir\n",
    "    BPS_fname = \"LF20_BPS_220.csv\"\n",
    "   \n",
    "    # Create empty dictionary\n",
    "    LF_ref_dicts = {}\n",
    "\n",
    "    # Get BPS reference dictionary\n",
    "    BPS_df = pd.read_csv(os.path.join(data_dir, BPS_fname))\n",
    "    LF_ref_dicts[\"BPS_NAME\"] = dict(BPS_df[['VALUE', 'BPS_NAME']].values)\n",
    "\n",
    "    return LF_ref_dicts\n",
    "                         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Append Features using Data Dictionaries__\n",
    "Append in selected features using the LANDFIRE data dictionaries.\n",
    "\n",
    "__Note:__ Items in feature_list must be in the source_layers dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(sample_points, feature_list = ['BPS_NAME']):\n",
    "    \"\"\"\n",
    "    Returns the sample_points layer with the features in feature_list appended. \n",
    "\n",
    "    Items in feature_list must be in the source_layers dictionary.  \n",
    "    \"\"\"\n",
    "    \n",
    "    LF_ref_dicts = read_ref_data()\n",
    "    \n",
    "    source_layers = {\n",
    "        'BPS_NAME' : 'BPS', \n",
    "    }\n",
    "\n",
    "    # Iterate through feature_list and append features to sample_points\n",
    "    for feature in feature_list:\n",
    "        sample_points[feature] = sample_points[source_layers[feature]].map(LF_ref_dicts[feature]).copy()\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Low Count Filter__\n",
    "Some classes have exceedingly low representation in the dataset. In order to run train_test_split, the target class must have a count greater than 1. The following will filter out classes with counts of 1. Given the size of the dataset excluding these classes will not detrimentally impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_count_filter(sample_points, target):  \n",
    "    \"\"\"\n",
    "    Returns modified sample_points dataframe. Classes in the target feature that have counts of 1 are removed. \n",
    "    \"\"\"  \n",
    "    # Group by FVT\n",
    "    LF22_FVT_counts = sample_points.groupby(target).count()\n",
    "\n",
    "    # Identify FVTs with low observation counts\n",
    "    low_count_FVT = LF22_FVT_counts[LF22_FVT_counts.iloc[:,0] < 5].index.tolist()\n",
    "\n",
    "    # Remove those FVTs from sample_points\n",
    "    sample_points = sample_points.loc[~sample_points[target].isin(low_count_FVT)]\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Prepare the Sample Points__\n",
    "Before training the model, the sample points need to be filtered. Filtering steps are:\n",
    "- Remove near points - defined as points that are less than 70m apart.\n",
    "- Remove Null points (-9999/-1111 values) - these will not be updated in the final raster. \n",
    "- Remove undisturbed values - the model is intended to update points that have been disturbed in the last 10 years. \n",
    "- Remove points with agricultural or developed FVT classes - these are updated using datasets not used for other classes. \n",
    "- Remove observations with classes that have very low representation (less than 5 observations in the class) in the dataset. This is done to allow train_test_split to run. \n",
    "- Optionally, join in additional features. feature_list may be modified to select which features to add in (see join_features for further details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(sample_point_fpath, target):\n",
    "    \"\"\"\n",
    "    Returns dataframe of processed sample points. Sample points are read in from shapefile. Processing includes filtering out null points and developed/agricultural FVTs as \n",
    "    well as appending additional features. \n",
    "    \"\"\"\n",
    "\n",
    "    # Read in gdf\n",
    "    sample_points = gpd.read_file(sample_point_fpath)\n",
    "    print(f\"Total Number of points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Filter out near points\n",
    "    sample_points = sample_points.loc[sample_points['NEAR_FID'] == -1]\n",
    "    print(f\"After removing near points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Drop unneeeded columns if present\n",
    "    sample_points = sample_points.drop(['Classified', 'GrndTruth', 'NEAR_FID', 'NEAR_DIST'], axis=1,\n",
    "                                       errors='ignore')\n",
    "    \n",
    "    # Remove observations with -9999/-1111 in any field \n",
    "    sample_points = sample_points.loc[~sample_points.isin([-1111, -9999]).any(axis=1)]\n",
    "    print(f\"After removing null points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Filter out points that weren't disturbed\n",
    "    sample_points = sample_points.loc[sample_points['LF22_FDST'] != 0]\n",
    "    print(f\"After removing undisturbed points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Join in additional features\n",
    "    sample_points = join_features(sample_points)\n",
    "\n",
    "    # Filter out agricultural and developed points\n",
    "    developed_fvt = list(range(20,33)) + list(range(2901,2906))\n",
    "    ag_fvt = [80, 81, 82] + list(range(2960, 2971))\n",
    "    fvt_filter = developed_fvt + ag_fvt\n",
    "    sample_points = sample_points.loc[~sample_points['LF22_FVT'].isin(fvt_filter)]\n",
    "\n",
    "    # Filter out classes in the target feature with very low counts\n",
    "    sample_points = low_count_filter(sample_points, target)\n",
    "\n",
    "    return sample_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Instantiate Estimators__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Histogram Based Gradient Boosting Classifier__\n",
    "Scikit-learn implementation of Histogram Based Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histGradientBoostingClassifier(categorical_feature_list, class_weight='Balanced', seed=1234):\n",
    "    \"\"\"\n",
    "    Returns specified histogram-based gradient boosting classifier. \n",
    "    \"\"\"\n",
    "\n",
    "    hgb_classifier = HistGradientBoostingClassifier(\n",
    "        categorical_features=categorical_feature_list,  # Natively handle categorical variables\n",
    "        class_weight=class_weight,\n",
    "        random_state=seed,\n",
    "        learning_rate=0.01,\n",
    "        max_iter=100\n",
    "    )\n",
    "\n",
    "    return hgb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Model__\n",
    "Trains and returns HGBC model provided data. Subsets the training data to a provided list of predictors and predicts the specified target feature.\n",
    "\n",
    "__Note__: Aspect, Elevation, and Slope are the only continuous LANDFIRE datasets, therefore any feature not in that list is assumed to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, predictors, target, model_type='HGBC'):\n",
    "    \"\"\"\n",
    "    Returns trained model of specified type given a set of predictors. \n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = \"balanced\"\n",
    "\n",
    "    # Get list of predictors for run\n",
    "    cat_features = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]  # Categorical features for HGBC models  \n",
    "    \n",
    "    # Separate training data predictors/response\n",
    "    y_train = train_data[target].copy()\n",
    "    X_train = train_data[predictors].copy()\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = histGradientBoostingClassifier(categorical_feature_list=cat_features, class_weight=class_weight)\n",
    "      \n",
    "    # Fit the model with the training data\n",
    "    print('Fitting Model...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the fit model\n",
    "    return model         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluate Model__\n",
    "Provided a trained model, test data, a set of predictors, and a target - return a list of predictions metrics on prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_data, predictors, target):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing 1. 'metrics' : a list of metrics quantifying model performance, and 2. 'predictions' : a list of predictions corresponding to \n",
    "    each observation in the sample_points data.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Separate the predictors from target\n",
    "    y_test = test_data[target].copy()\n",
    "    x_test = test_data[predictors].copy()\n",
    "\n",
    "    # Perform prediction\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Get metrics\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    bal_acc = round(balanced_accuracy_score(y_test, y_pred),3)\n",
    "    recall = round(recall_score(y_test, y_pred, average='macro'), 3)\n",
    "    precision = round(precision_score(y_test, y_pred, average='macro'), 3)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='macro'), 3)\n",
    "\n",
    "    print(f\"Predictors: {predictors}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics\" : [accuracy, bal_acc, recall, precision, f1, predictors],\n",
    "        \"predictions\" : y_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Main__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVT: [2016, 2064, 2066, 2099, 2128, 2177, 2184, 2211, 2219, 2260, 2567, 2910, 2911]\n",
      "After removing low count LF22_FVT: (160346, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n",
      "Fitting Model...\n",
      "Model written to: C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\\models\\LF22_FVT_HGBC_model_2024-05-16_11-30-32\n"
     ]
    }
   ],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "\n",
    "# Define predictors and target\n",
    "predictors = ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "# Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "# Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  \n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=sample_points, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Save model \n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "model_fpath = os.path.join(paths_dict['model_dir'], model_fname)\n",
    "dump(model, model_fpath)\n",
    "print(f\"Model written to: {model_fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 200390\n",
      "After removing near points: 196097\n",
      "After removing null points: 163092\n",
      "After removing undisturbed points: 163092\n",
      "Low count LF22_FVT: [2016, 2064, 2066, 2099, 2128, 2177, 2184, 2211, 2219, 2260, 2567, 2910, 2911]\n",
      "After removing low count LF22_FVT: (163057, 31)\n",
      "Sample Points Attributes: Index(['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF16_EVT', 'LF14_EVT', 'LF22_FDST',\n",
      "       'ZONE', 'BPS', 'ASPECT', 'SLOPE', 'ELEVATION', 'LF20_EVT', 'LF20_SCla',\n",
      "       'LF20_FVH', 'LF20_FVC', 'LF22_FVH', 'LF22_FVC', 'LF16_FVH', 'LF16_FVC',\n",
      "       'PRED_FVT', 'LF22_F40', 'BPS_FRG_NE', 'geometry', 'BPS_NAME', 'BPS_FRG',\n",
      "       'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N',\n",
      "       'EVT_CLASS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train, test = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Model...\n",
      "Score on the Training Set: \n",
      "Predictors: ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
      "Accuracy: 0.733\n",
      "Balanced Accuracy: 0.899\n",
      "Recall: 0.899\n",
      "Precision: 0.68\n",
      "F1: 0.756\n",
      "Score on the test set: \n",
      "Predictors: ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
      "Accuracy: 0.67\n",
      "Balanced Accuracy: 0.558\n",
      "Recall: 0.558\n",
      "Precision: 0.462\n",
      "F1: 0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcco573\\AppData\\Local\\miniconda3\\envs\\geospatial_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test set\n",
    "test[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=f'{target}_model_results')\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
