{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "\n",
    "__Author__: Pat McCornack\n",
    "\n",
    "__Date__: 4/9/24\n",
    "\n",
    "__Purpose__: Develop model to update FVT using most recent FDist data. \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from joblib import dump\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Paths\n",
    "Define set of filepaths to conveniently switch between working off local files or the PNNL drive. Set active_data_dir to either local_data_dir or pnnl_data_dir depending on which you're working off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_root_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\"\n",
    "pnnl_root_dir = r\"\\\\pnl\\projects\\BPAWildfire\\data\\Landfire\\fuels_modeling\\Fuel Attributes Model\"\n",
    "# Define which data directory to work off of\n",
    "active_root_dir = local_root_dir\n",
    "\n",
    "paths_dict = {\n",
    "    \"out_base_dir\" : os.path.join(active_root_dir, r\"model_outputs\\tabular\"),  # Where to save outputs \n",
    "    \"ref_data_dir\" : os.path.join(active_root_dir, r\"..\\LF_raster_data\\_tables\"),  # Location of LF csvs (e.g. LF22_FVT_230.csv)\n",
    "    \"sample_points_dir\" : os.path.join(active_root_dir, r\"data\\sample_points\"),  # Location of shapefile to train model on\n",
    "    \"sample_points_fname\" : r\"sample_points_4-17-24_200k_Disturbed.shp\",  # Name of shapefile to train model on\n",
    "    \"runs_dict_fpath\" : os.path.join(active_root_dir, r\"data\\runs_dict.csv\"),  # Used to test different sets of predictors \n",
    "    \"model_dir\" : os.path.join(active_root_dir, \"models\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Functions__\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Create a Directory to Output Modeling Results__\n",
    "Names the output directory using the datetime that the script was run. \n",
    "Returns the name of the directory. The returned directory is used to output the trained model and/or results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(base_dir, new_dir_name='model_results'):\n",
    "        \"\"\"\n",
    "        Returns path to a directory created at the specified base_dir location. \n",
    "\n",
    "        The name of the created directory can optionally be specified using the dir_name argument. \n",
    "        \"\"\"\n",
    "\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        output_dir = os.path.join(base_dir, new_dir_name + \"_\" + datetime)\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Categorical Variable Encoder__\n",
    "Categorical variables must be encoded before being used in some model types. Note that due to HGBC's native encoding support this function is not needed when using that model type.\n",
    "\n",
    "__Note:__ Aspect, elevation, and slope are the only LANDFIRE variables of interest that are not categorical, which is why they're removed from the predictor list before the data is encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targetEncoder(train, test, predictors, target):\n",
    "    \"\"\"\n",
    "    Returns the train and test sets with the categorical predictors encoded. The predictors argument is a list of features being used as \n",
    "    predictors in the model, while target is the name of target feature. \n",
    "    \"\"\"\n",
    "    # Subset to attributes of interest\n",
    "    attributes = predictors + [target]\n",
    "    train = train[attributes]\n",
    "    test = test[attributes]\n",
    "\n",
    "    # Separate out the predictors from the target and remove continuous variables\n",
    "    cat_predictors = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]  # Drops continuous variables\n",
    "\n",
    "    # Encode the features\n",
    "    enc = TargetEncoder(target_type=\"multiclass\", random_state=1234).set_output(transform=\"pandas\")\n",
    "    enc.fit(train[cat_predictors], train[target])  # Fit the encoder\n",
    "    train_trans = enc.transform(train[cat_predictors])  # Encode the train data\n",
    "    test_trans = enc.transform(test[cat_predictors])  # Encode the test data\n",
    "\n",
    "    # Replace the features with encoded features\n",
    "    train = train.drop(cat_predictors, axis=1)\n",
    "    train = pd.concat([train, train_trans], axis=1)\n",
    "\n",
    "    test = test.drop(cat_predictors, axis=1)\n",
    "    test = pd.concat([test, test_trans], axis=1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Standardize Data__\n",
    "This is primarily used to standardize data for the MLP since tree-based methods aren't sensitive to data scale. By default it is set to standardize only topographic variables given that they are the only continuous LANDFIRE features of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(train, test, features=['SLOPE', 'ASPECT', 'ELEVATION']):\n",
    "    \"\"\"\n",
    "    Return train and test datasets with the features in the 'features' argument having been standardized.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the scaler to the training data\n",
    "    scaler = StandardScaler().fit(train[features])\n",
    "\n",
    "    # Scale the training data\n",
    "    train[features] = scaler.transform(train[features])\n",
    "\n",
    "    # Scale the test data\n",
    "    test[features] = scaler.transform(test[features])\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Get List of Lists of Predictor Sets__\n",
    "In the interest of assessing model performance using multiple sets of predictors, this function utilizes an excel file to generate a list of lists of sets of predictors that can be iterated through the model train/evaluation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runs(runs_dict_fpath=paths_dict['runs_dict_fpath']):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, where each list is a set of predictors to evaluate model performance with. \n",
    "\n",
    "    runs_dict_fpath is the path to an excel file where each column corresponds to a feature, and each row corresponds to a set\n",
    "    of predictors. If a cell contains a '1', then that feature is included that set of predictors.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(runs_dict_fpath)\n",
    "    \n",
    "    # Drop the \"Run\" column\n",
    "    df = df.drop('Run', axis=1)\n",
    "    \n",
    "    runs = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        runs.append(row.loc[row==1].index.tolist())\n",
    "    \n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate Combinations of Features of Interest__\n",
    "Alternatively, automatically generate all possible combinations of a set of potential predictor features to evaluate. \n",
    "\n",
    "__Caution__: The size of the list of lists of predictor sets can quickly become unmanageable when too many features of interest are included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(attributes, target):\n",
    "    \"\"\"\n",
    "    Returns a list of lists of all possible combinations of the list of attributes provided. \n",
    "    \"\"\"\n",
    "\n",
    "    attributes = [x for x in attributes if x not in [target]]  # Drop the target variable\n",
    "    predictors = []\n",
    "\n",
    "    for i in range(len(attributes)):\n",
    "        combs = [list(x) for x in combinations(attributes, i)]\n",
    "        for item in combs:\n",
    "            predictors.append(item)\n",
    "\n",
    "    return predictors[1:]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Pre-Process the Data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Create Data Dictionaries to Append Features__\n",
    "Some features are separate attributes of the LANDFIRE dataset (e.g. BPS Fire Regime) and others are useful for results analysis (e.g. FDst attributes). These can be mapped to points using LANDFIRE CSVs. The below creates dictionaries to perform that mapping. \n",
    "\n",
    "This function is called by join_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref_data(ref_data_dir=paths_dict[\"ref_data_dir\"]):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of dictionaries of mappings between LANDFIRE raster values and other attributes associated with those values. \n",
    "    \"\"\"\n",
    "    data_dir = ref_data_dir\n",
    "    BPS_fname = \"LF20_BPS_220.csv\"\n",
    "    LF16_EVT_fname = \"LF16_EVT_200.csv\"\n",
    "    LF22_FDST_fname = \"LF22_FDST_230.csv\"\n",
    "    LF22_FVT_fname = \"LF22_FVT_230.csv\"\n",
    "    LF22_EVT_fname = \"LF22_EVT_230.csv\"\n",
    "    \n",
    "    # Create empty dictionary\n",
    "    LF_ref_dicts = {}\n",
    "\n",
    "    # Get BPS reference dictionary\n",
    "    BPS_df = pd.read_csv(os.path.join(data_dir, BPS_fname))\n",
    "    LF_ref_dicts[\"BPS_NAME\"] = dict(BPS_df[['VALUE', 'BPS_NAME']].values)\n",
    "    LF_ref_dicts[\"BPS_FRG\"] = dict(BPS_df[['VALUE', 'FRG_NEW']].values)\n",
    "\n",
    "    # Get FDST reference dictionaries \n",
    "    FDST_df = pd.read_csv(os.path.join(data_dir, LF22_FDST_fname ))\n",
    "    LF_ref_dicts[\"FDST_TYPE\"] = dict(FDST_df[['VALUE', 'D_TYPE']].values)\n",
    "    LF_ref_dicts[\"FDST_SEV\"] = dict(FDST_df[['VALUE', 'D_SEVERITY']].values)\n",
    "    LF_ref_dicts[\"FDST_TSD\"] = dict(FDST_df[['VALUE', 'D_TIME']].values)\n",
    "\n",
    "    # Get EVT reference dictionaries\n",
    "    EVT_df = pd.read_csv(os.path.join(data_dir, LF16_EVT_fname))\n",
    "    LF_ref_dicts[\"EVT_PHYS\"] = dict(EVT_df[['VALUE', 'EVT_PHYS']].values)\n",
    "    LF_ref_dicts[\"EVT_GP_N\"] = dict(EVT_df[['VALUE', 'EVT_GP_N']].values) \n",
    "    LF_ref_dicts[\"EVT_CLASS\"] = dict(EVT_df[['VALUE', 'EVT_CLASS']].values) \n",
    "\n",
    "    # Get FVT reference dictionaries\n",
    "    LF22_FVT_df = pd.read_csv(os.path.join(data_dir, LF22_FVT_fname))\n",
    "    LF_ref_dicts['LF22_FVT'] = dict(LF22_FVT_df[['VALUE', 'EVT_FUEL_N']].values)\n",
    "    \n",
    "    ## Map FVT to EVT Groupings\n",
    "    LF22_EVT_df = pd.read_csv(os.path.join(data_dir, LF22_EVT_fname))\n",
    "    LF_ref_dicts['FVT_EVT'] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_FUEL_N']].values)\n",
    "    LF_ref_dicts[\"FVT_PHYS\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_PHYS']].values)\n",
    "    LF_ref_dicts[\"FVT_GP_N\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_GP_N']].values) \n",
    "    LF_ref_dicts[\"FVT_CLASS\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_CLASS']].values) \n",
    "    LF_ref_dicts[\"FVT_LF\"] = dict(LF22_EVT_df[['EVT_FUEL', 'EVT_LF']].values)\n",
    "\n",
    "    return LF_ref_dicts\n",
    "                         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Append Features using Data Dictionaries__\n",
    "Append in selected features using the LANDFIRE data dictionaries.\n",
    "\n",
    "__Note:__ Items in feature_list must be in the source_layers dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(sample_points, feature_list = ['BPS_NAME', 'BPS_FRG', 'FDST_TYPE', 'FDST_SEV', 'FDST_TSD', 'EVT_PHYS', 'EVT_GP_N', 'EVT_CLASS']):\n",
    "    \"\"\"\n",
    "    Returns the sample_points layer with the features in feature_list appended. \n",
    "\n",
    "    Items in feature_list must be in the source_layers dictionary.  \n",
    "    \"\"\"\n",
    "    \n",
    "    LF_ref_dicts = read_ref_data()\n",
    "    \n",
    "    source_layers = {\n",
    "        'BPS_NAME' : 'BPS', \n",
    "        'BPS_FRG' : 'BPS',\n",
    "        'FDST_TYPE' : 'LF22_FDST',\n",
    "        'FDST_SEV' : 'LF22_FDST', \n",
    "        'FDST_TSD' : 'LF22_FDST', \n",
    "        'EVT_PHYS' : 'LF16_EVT',\n",
    "        'EVT_GP_N' : 'LF16_EVT',\n",
    "        'EVT_CLASS' : 'LF16_EVT'\n",
    "    }\n",
    "\n",
    "    # Iterate through feature_list and append features to sample_points\n",
    "    for feature in feature_list:\n",
    "        sample_points[feature] = sample_points[source_layers[feature]].map(LF_ref_dicts[feature]).copy()\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Low Count Filter__\n",
    "Some classes have exceedingly low representation in the dataset. In order to run train_test_split, the target class must have a count greater than 1. The following will filter out classes with counts of 1. Given the size of the dataset excluding these classes will not detrimentally impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_count_filter(sample_points, target):  \n",
    "    \"\"\"\n",
    "    Returns modified sample_points dataframe. Classes in the target feature that have counts of 1 are removed. \n",
    "    \"\"\"  \n",
    "    # Group by FVT\n",
    "    LF22_FVT_counts = sample_points.groupby(target).count()\n",
    "\n",
    "    # Identify FVTs with low observation counts\n",
    "    low_count_FVT = LF22_FVT_counts[LF22_FVT_counts.iloc[:,0] < 5].index.tolist()\n",
    "\n",
    "    # Remove those FVTs from sample_points\n",
    "    sample_points = sample_points.loc[~sample_points[target].isin(low_count_FVT)]\n",
    "    print(f\"Low count {target}: {low_count_FVT}\")\n",
    "    print(f\"After removing low count {target}: {sample_points.shape}\")\n",
    "    print(f\"Sample Points Attributes: {sample_points.columns}\")\n",
    "\n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Prepare the Sample Points__\n",
    "Before training the model, the sample points need to be filtered. Filtering steps are:\n",
    "- Remove near points - defined as points that are less than 70m apart.\n",
    "- Remove Null points (-9999/-1111 values) - these will not be updated in the final raster. \n",
    "- Remove undisturbed values - the model is intended to update points that have been disturbed in the last 10 years. \n",
    "- Remove points with agricultural or developed FVT classes - these are updated using datasets not used for other classes. \n",
    "- Remove observations with classes that have very low representation (less than 5 observations in the class) in the dataset. This is done to allow train_test_split to run. \n",
    "- Optionally, join in additional features. feature_list may be modified to select which features to add in (see join_features for further details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(sample_point_fpath, target):\n",
    "    \"\"\"\n",
    "    Returns dataframe of processed sample points. Sample points are read in from shapefile. Processing includes filtering out null points and developed/agricultural FVTs as \n",
    "    well as appending additional features. \n",
    "    \"\"\"\n",
    "\n",
    "    # Read in gdf\n",
    "    sample_points = gpd.read_file(sample_point_fpath)\n",
    "    print(f\"Total Number of points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Filter out near points\n",
    "    sample_points = sample_points.loc[sample_points['NEAR_FID'] == -1]\n",
    "    print(f\"After removing near points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Drop unneeeded columns if present\n",
    "    sample_points = sample_points.drop(['Classified', 'GrndTruth', 'NEAR_FID', 'NEAR_DIST'], axis=1,\n",
    "                                       errors='ignore')\n",
    "    \n",
    "    # Remove observations with -9999/-1111 in any field \n",
    "    sample_points = sample_points.loc[~sample_points.isin([-1111, -9999]).any(axis=1)]\n",
    "    print(f\"After removing null points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Filter out points that weren't disturbed\n",
    "    sample_points = sample_points.loc[~(sample_points['LF22_FDST'] == 0)]\n",
    "    print(f\"After removing undisturbed points: {sample_points.shape[0]}\")\n",
    "\n",
    "    # Join in additional features\n",
    "    sample_points = join_features(sample_points)\n",
    "\n",
    "    # Filter out agricultural and developed points\n",
    "    developed_fvt = list(range(20,33)) + list(range(2901,2906))\n",
    "    ag_fvt = [80, 81, 82] + list(range(2960, 2971))\n",
    "    fvt_filter = developed_fvt + ag_fvt\n",
    "    sample_points = sample_points.loc[~sample_points['LF22_FVT'].isin(fvt_filter)]\n",
    "\n",
    "    # Filter out classes in the target feature with very low counts\n",
    "    sample_points = low_count_filter(sample_points, target)\n",
    "\n",
    "    return sample_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Instantiate Estimators__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Histogram Based Gradient Boosting Classifier__\n",
    "Scikit-learn implementation of Histogram Based Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histGradientBoostingClassifier(categorical_feature_list, class_weight='Balanced', seed=1234):\n",
    "    \"\"\"\n",
    "    Returns specified histogram-based gradient boosting classifier. \n",
    "    \"\"\"\n",
    "\n",
    "    hgb_classifier = HistGradientBoostingClassifier(\n",
    "        categorical_features=categorical_feature_list,  # Natively handle categorical variables\n",
    "        class_weight=class_weight,\n",
    "        random_state=seed,\n",
    "        learning_rate=0.01,\n",
    "        max_iter=100\n",
    "    )\n",
    "\n",
    "    return hgb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Catboost Gradient Boosting__\n",
    "Catboost implementation of Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catBoost_hgbc(cat_features):\n",
    "    \"\"\"\n",
    "    Returns specified gradient boosting classifier from catBoost library. \n",
    "    \"\"\"\n",
    "    cb_hgbc = CatBoostClassifier(\n",
    "        iterations=2000, \n",
    "        learning_rate=0.01,\n",
    "        loss_function=\"MultiClass\",\n",
    "        #boosting_type=\"Plain\",\n",
    "        cat_features=cat_features,  # List of categorical features\n",
    "        auto_class_weights='Balanced', # Used to handle class imbalance\n",
    "        random_seed=1234,\n",
    "        task_type='GPU'  # Train the model on GPU\n",
    "    )\n",
    "\n",
    "    return cb_hgbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Random Forest Classifier__\n",
    "Scikit-learn implementation Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestClassifier(n_est=100, min_samples_leaf=50, bootstrap=True, oob_score=True, \n",
    "                           n_jobs=-1, random_state=1234, max_features='auto', class_weight=None):\n",
    "    \"\"\"\n",
    "    Returns specified Random Forest Classifier. \n",
    "    \"\"\"\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=n_est,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        bootstrap=bootstrap,\n",
    "        oob_score=oob_score,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        max_features=max_features,\n",
    "        class_weight=class_weight,\n",
    "        max_depth = None\n",
    "    )\n",
    "\n",
    "    return rf_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Multi-Layer Perceptron__\n",
    "Scikit-learn implementation of Multi-Layer Perceptron. Note that this has not been tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiLayerPerceptron():\n",
    "    \"\"\"\n",
    "    Returns specified Multi Layer Perceptron.\n",
    "    \"\"\"\n",
    "    clf = MLPClassifier(\n",
    "        max_iter = 10000,\n",
    "        activation = 'relu',  # 'relu' is the default\n",
    "        hidden_layer_sizes = (256, 128, 64)\n",
    "    )\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train Model__\n",
    "Trains and returns HGBC model provided data. Subsets the training data to a provided list of predictors and predicts the specified target feature.\n",
    "\n",
    "__Note__: Aspect, Elevation, and Slope are the only continuous LANDFIRE datasets, therefore any feature not in that list is assumed to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, predictors, target, model_type='HGBC'):\n",
    "    \"\"\"\n",
    "    Returns trained model of specified type given a set of predictors. \n",
    "    \"\"\"\n",
    "\n",
    "    class_weight = \"balanced\"\n",
    "\n",
    "    # Get list of predictors for run\n",
    "    cat_features = [x for x in predictors if x not in ['ASPECT', 'ELEVATION', 'SLOPE']]  # Categorical features for HGBC models  \n",
    "    \n",
    "    # Setting the data type to categorical speeds up model training for catBoost\n",
    "    if model_type == \"catBoost\":\n",
    "        train_data[cat_features] = train_data[cat_features].astype(\"category\")\n",
    "\n",
    "    # Separate training data predictors/response\n",
    "    y_train = train_data[target].copy()\n",
    "    X_train = train_data[predictors].copy()\n",
    "\n",
    "    # Fit selected model_type\n",
    "    if model_type == 'RF':\n",
    "        model = randomForestClassifier(n_est=100, min_samples_leaf=50, bootstrap=True, oob_score=True,\n",
    "                                            n_jobs=-1, random_state=1234, max_features='sqrt', class_weight=class_weight)\n",
    "    elif model_type == 'HGBC':\n",
    "        model = histGradientBoostingClassifier(categorical_feature_list=cat_features, class_weight=class_weight)\n",
    "    \n",
    "    elif model_type == 'catBoost':\n",
    "        model = catBoost_hgbc(cat_features=cat_features)\n",
    "\n",
    "    elif model_type == 'MLP':\n",
    "        print('Defining MLP...')\n",
    "        model = multiLayerPerceptron()\n",
    "    \n",
    "    # Fit the model with the training data\n",
    "    print('Fitting Model...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the fit model\n",
    "    return model         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Evaluate Model__\n",
    "Provided a trained model, test data, a set of predictors, and a target - return a list of predictions metrics on prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_data, predictors, target, model_type='HGBC'):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing 1. 'metrics' : a list of metrics quantifying model performance, and 2. 'predictions' : a list of predictions corresponding to \n",
    "    each observation in the sample_points data.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Separate the predictors from target\n",
    "    y_test = test_data[target].copy()\n",
    "    x_test = test_data[predictors].copy()\n",
    "\n",
    "    # Perform prediction\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Get metrics\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    bal_acc = round(balanced_accuracy_score(y_test, y_pred),3)\n",
    "    recall = round(recall_score(y_test, y_pred, average='macro'), 3)\n",
    "    precision = round(precision_score(y_test, y_pred, average='macro'), 3)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='macro'), 3)\n",
    "\n",
    "    print(f\"Predictors: {predictors}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics\" : [accuracy, bal_acc, recall, precision, f1, predictors],\n",
    "        \"predictions\" : y_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Results Evaluation Functions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Plot the Class Distribution__\n",
    "Plots the distribution of the variable of interest. Can be set to display the raw counts for each class or the percentage of the dataste that the class accounts for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, group_var, metric='Count', sort=True, title=f'Distribution', figsize=(15, 50)):\n",
    "    \"\"\"\n",
    "    Plots the distribution of group_var. 'metric' can be set to either 'Count' or 'Percent' to change the display type. \n",
    "    The 'sort' argument specifies whether to sort the display in order from most common to least common. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get count of target class\n",
    "    df_gp = df.groupby(group_var).count().reset_index()\n",
    "    df_gp = df_gp.iloc[:, 0:2].rename(columns={df_gp.columns[1] : 'Count'})\n",
    "\n",
    "    # Get classes proportional representation\n",
    "    df_gp['Percent'] = df_gp['Count'] / df_gp['Count'].sum() * 100\n",
    "    df_gp[group_var] = df_gp[group_var].astype('str')  # For plotting change FVT to str\n",
    "    if sort == True: \n",
    "        df_gp = df_gp.sort_values(by=\"Count\", ascending=True)\n",
    "\n",
    "    # Visualize the distribution\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    hbars = ax.barh(df_gp[group_var],\n",
    "                df_gp[metric],\n",
    "                align='center')\n",
    "    ax.set(title=title,\n",
    "        xlabel= f'{metric}', \n",
    "        ylabel=f'{group_var}')  \n",
    "    ax.bar_label(hbars, fmt=\" %.2f\")\n",
    "    \n",
    "    # Remove whitespace at margins\n",
    "    plt.ylim(ax.patches[0].get_y(), ax.patches[-1].get_y() + ax.patches[-1].get_height()+0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Plot Confusion Matrix__\n",
    "Generates a confusion matrix of true labels vs. predicted labels.\n",
    "Normalization options are: \n",
    "- 'true' : Normalizes over the rows.\n",
    "- 'pred' : Normalizes over the columns. \n",
    "- 'all' : Normalizes over the entire population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix\n",
    "def plot_cm(y_true, y_pred, normalize=None, figsize=10, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix of true values (y_true) against predicted values (y_pred). Can be normalized by setting \n",
    "    normalize='true'/'pred'/'all'.\n",
    "    \"\"\"\n",
    "\n",
    "    np.set_printoptions(precision=2) \n",
    "\n",
    "    # Define title\n",
    "    title = f\"{title}, normalize = {normalize}\"\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                                   cmap=plt.cm.Blues,\n",
    "                                                   normalize=normalize,\n",
    "                                                   xticks_rotation='vertical'\n",
    "                                                   )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    # Set plot size\n",
    "    fig = disp.ax_.get_figure()\n",
    "    fig.set_figwidth(figsize)\n",
    "    fig.set_figheight(figsize)\n",
    "\n",
    "    # Display results\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Main__\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __FVT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVT'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_point_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF20_FVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH']\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, features=predictors)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test set\n",
    "test[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=f'{target}_model_results')\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test set using the FVT predictions. This will then be fed into the FVC/FVH models to see how replacing the true FVT values with predicted values will impact the FVC/FVH predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvt_test = test.copy()\n",
    "fvt_test[\"LF22_FVT_Predictions\"] = results['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __FVC__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVC'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.7\n",
    "test_frac = 0.3\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath =  os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVC', 'LF16_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test set\n",
    "test[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=f'{target}_model_results')\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on multiple sets of predictors\n",
    "Used to compare/contrast the performance of the model using various sets of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple sets of predictors to compare\n",
    "\n",
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors - includes target and predictors\n",
    "predictors_list = [\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE'],\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF16_FVC', 'LF22_FDST', 'ZONE'],\n",
    "    ['LF22_FVT', 'LF20_FVC', 'LF16_FVC', 'LF22_FDST', 'ZONE', 'BPS_NAME'],\n",
    "    ['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF20_FVC', 'LF16_FVC', 'LF20_FVH', 'LF16_FVH', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME']\n",
    "]\n",
    "\n",
    "# Define results DF\n",
    "results_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'predictors', 'model_type', 'train/test'])\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "for predictors in predictors_list: \n",
    "    # Train model\n",
    "    model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "    # Evaluate Model\n",
    "    print(\"Score on the Training Set: \")\n",
    "    results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "    results_df.loc[len(results_df)] = results['metrics'] + [f'{model_type}', 'train']\n",
    "\n",
    "    print(\"Score on the test set: \")\n",
    "    results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)\n",
    "    results_df.loc[len(results_df)] = results['metrics'] + [f'{model_type}', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __FVC with Predicted FVT__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVC'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath = os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and run model\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "## Define set of predictors - includes target and predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVC', 'LF22_FDST', 'ZONE']\n",
    "\n",
    "## Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "## Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "## Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on test set with predicted FVTs\")\n",
    "results = eval_model(model=model, test_data=fvt_test, predictors=predictors, target=target, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test_points set\n",
    "test_points[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=paths_dict['new_dir_name'])\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test_points.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  __FVH__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and perform train/test split.\n",
    "seed = 1234\n",
    "target = 'LF22_FVH'\n",
    "\n",
    "## Define train/test split proportion\n",
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "\n",
    "## Define path to sample points\n",
    "sample_points_fpath = os.path.join(paths_dict['sample_points_dir'], paths_dict['sample_points_fname'])\n",
    "\n",
    "## Read in and prepare data\n",
    "sample_points = data_prep(sample_points_fpath, target)\n",
    "\n",
    "## Perform Train/Test Split\n",
    "train_points, test_points = train_test_split(sample_points, train_size=train_frac, test_size=test_frac, \n",
    "                               random_state=seed, shuffle=True, stratify=sample_points[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC / catBoost / MLP\n",
    "\n",
    "# Define set of predictors - includes target and predictors\n",
    "predictors = ['LF22_FVT', 'LF20_FVH', 'LF16_FVH', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "#predictors = ['LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'LF20_FVC', 'LF20_FVH',  target]\n",
    "\n",
    "# Extra processing if using Random Forest or Multi-Layer Perceptron\n",
    "if model_type == 'RF' or model_type == 'MLP':\n",
    "    train, test = targetEncoder(train_points, test_points, predictors, target)\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    # For multi-layer perceptron, encoded data needs to be standardized\n",
    "    if model_type == 'MLP':\n",
    "        train, test = standardize_data(train, test, predictors, target)\n",
    "else:\n",
    "    train, test = train_points, test_points\n",
    "\n",
    "# Train model\n",
    "model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "# Evaluate Model\n",
    "print(\"Score on the Training Set: \")\n",
    "results = eval_model(model=model, test_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "print(\"Score on the test set: \")\n",
    "results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out results\n",
    "## Concatenate predictions to test_points set\n",
    "test_points[f'PRED_{target}'] = results['predictions']\n",
    "\n",
    "## Create dataframe of metrics\n",
    "metrics_df = pd.DataFrame(columns=['accuracy', 'balanced accuracy', 'recall', 'precision' ,'f1 score', 'attributes', 'model_type'])\n",
    "metrics_df.loc[0] = results['metrics'] + [model_type]\n",
    "\n",
    "## Save out the dataframes\n",
    "out_dir = make_dir(paths_dict['out_base_dir'], new_dir_name=paths_dict['new_dir_name'])\n",
    "preds_out_fname = f\"Predictions_{target}_{model_type}.csv\"\n",
    "test_points.to_csv(os.path.join(out_dir, preds_out_fname))\n",
    "\n",
    "metrics_out_fname = f\"Metrics_{target}_{model_type}.csv\"\n",
    "metrics_df.to_csv(os.path.join(out_dir, metrics_out_fname))\n",
    "\n",
    "# Save out model to results dir\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_fname = f\"{target}_{model_type}_model_{datetime}\"\n",
    "dump(model, os.path.join(out_dir, model_fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a list of models\n",
    "Run multiple sets of predictors to evaluate against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of model to be used\n",
    "model_type = 'HGBC'  # RF / HGBC\n",
    "\n",
    "# Generate combinations of potential predictor sets\n",
    "##predictors = ['LF22_FVT', 'LF20_FVT', 'LF16_FVT', 'LF14_EVT', 'LF22_FDST', 'ZONE', 'ASPECT', 'SLOPE', 'ELEVATION', 'BPS_NAME', 'EVT_GP_N', 'EVT_PHYS', 'EVT_CLASS']\n",
    "##predictors_list = generate_combinations(predictors, 'LF22_FVT')\n",
    "\n",
    "# Alternatively, get list of lists of predictors sets from file \n",
    "##predictors_list = get_runs()\n",
    "\n",
    "# Alternatively, define a list of lists of attribute combinations\n",
    "predictors_list = [\n",
    "    ['ASPECT', 'ELEVATION', 'SLOPE', 'LF22_FDST', 'ZONE'],\n",
    "    ['ASPECT', 'ELEVATION', 'SLOPE', 'LF22_FDST', 'ZONE'],\n",
    "    ['ASPECT', 'ELEVATION', 'SLOPE', 'LF20_EVT', 'LF22_FDST', 'ZONE'],\n",
    "    ['LF20_EVT', 'LF22_FDST', 'ZONE', 'BPS_NAME']\n",
    "\n",
    "]\n",
    "\n",
    "# Dataframe to store results\n",
    "result_df = pd.DataFrame(columns=['Accuracy', 'Balanced_Accuracy', 'Recall', 'Precision', 'F1', 'Predictors'])\n",
    "\n",
    "# Iterate through combinations of predictors\n",
    "for predictors in predictors_list:\n",
    "    # Train model\n",
    "    model = train_model(train_data=train, predictors=predictors, target=target, model_type=model_type)\n",
    "\n",
    "    # Evaluate Model\n",
    "    results = eval_model(model=model, test_data=test, predictors=predictors, target=target, model_type=model_type)['metrics']\n",
    "\n",
    "    result_df.loc[len(result_df)] = results\n",
    "\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
