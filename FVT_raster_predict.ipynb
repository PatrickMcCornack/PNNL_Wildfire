{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "__Author__: Pat McCornack\n",
    "\n",
    "__Date__: 3/21/2024\n",
    "\n",
    "__Purpose:__ Script to apply a trained model to predict FBFM40 values. Outputs a raster of predicted FBFM40 values across the bpa service territory. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from joblib import load\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "local_root_dir = r\"C:\\Users\\mcco573\\OneDrive - PNNL\\Documents\\_Projects\\BPA Wildfire\\Fuel Attributes Model\"\n",
    "pnnl_root_dir = r\"\\\\pnl\\projects\\BPAWildfire\\data\\Landfire\\fuels_modeling\\Fuel Attributes Model\"\n",
    "\n",
    "active_root_dir = pnnl_root_dir\n",
    "\n",
    "\n",
    "paths_dict = {\n",
    "    'data_dir' : os.path.join(active_root_dir, r'..\\LF_raster_data\\bpa_service_territory'),\n",
    "    'out_base_dir' : os.path.join(active_root_dir, r'model_outputs\\geospatial'),\n",
    "    'new_dir_name' : 'LF22_Pred_FVT',  # Name of created directory to hold results - will have datetime appended to it\n",
    "    'ref_data_dir' : os.path.join(active_root_dir, r'..\\LF_raster_data\\_tables'),\n",
    "    'model_dir' : os.path.join(active_root_dir, 'models'),\n",
    "    'model_fname' : \"LF22_FVT_HGBC_model_2024-05-10_16-49-07\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Functions__\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Make Directory__\n",
    "Creates a directory where data will be output - labeled with the datetime that the script is run. Returns name of directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(base_dir, new_dir_name):\n",
    "        \"\"\"\n",
    "        Create a directory named using the current datetime.\n",
    "        Returns:\n",
    "        - Name of the directory as a string\n",
    "        \"\"\"\n",
    "\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        output_dir = os.path.join(base_dir, new_dir_name + \"_\" + datetime)\n",
    "\n",
    "        os.makedirs(output_dir)\n",
    "        return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Feature Engineering__\n",
    "Use values from the rasters to create new features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Reference Data\n",
    "Use the LANDFIRE csv tables to create dictionaries to be used to map raster layer values to other features.\n",
    "1. Separate LF22_FDST into separate features for type, severity, and time since disturbance.\n",
    "2. Map the BPS value to BPS_NAME in order to reduce cardinality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_data_dir = paths_dict['ref_data_dir']\n",
    "def read_ref_data(ref_data_dir=ref_data_dir):\n",
    "    \"\"\"\n",
    "    Creates dictionaries from LF attributes tables that are used to map layer values to other features. Returns a dictionary of dictionaries. \n",
    "    \"\"\"\n",
    "    data_dir = ref_data_dir\n",
    "    BPS_fname = \"LF20_BPS_220.csv\"\n",
    "    LF22_FDST_fname = \"LF22_FDST_230.csv\"\n",
    "\n",
    "    # Create empty dictionary\n",
    "    LF_ref_dicts = {}\n",
    "\n",
    "    # Get BPS reference dictionary\n",
    "    BPS_df = pd.read_csv(os.path.join(data_dir, BPS_fname))\n",
    "    LF_ref_dicts[\"BPS_NAME\"] = dict(BPS_df[['VALUE', 'BPS_NAME']].values)\n",
    "    LF_ref_dicts[\"BPS_FRG\"] = dict(BPS_df[['VALUE', 'FRG_NEW']].values)\n",
    "\n",
    "    # Get FDST reference dictionaries \n",
    "    FDST_df = pd.read_csv(os.path.join(data_dir, LF22_FDST_fname ))\n",
    "    LF_ref_dicts[\"FDST_TYPE\"] = dict(FDST_df[['VALUE', 'D_TYPE']].values)\n",
    "    LF_ref_dicts[\"FDST_SEV\"] = dict(FDST_df[['VALUE', 'D_SEVERITY']].values)\n",
    "    LF_ref_dicts[\"FDST_TSD\"] = dict(FDST_df[['VALUE', 'D_TIME']].values)\n",
    "\n",
    "    return LF_ref_dicts                        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Features\n",
    "Use dictionaries created using read_ref_data to map raster values to new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(df, feature_list = ['BPS_NAME', 'FDST_TYPE', 'FDST_SEV', 'FDST_TSD']):\n",
    "    \"\"\"\n",
    "    Joins in additional features using LF attribute tables. \n",
    "    \"\"\"\n",
    "    \n",
    "    LF_ref_dicts = read_ref_data()\n",
    "    \n",
    "    source_layers = {\n",
    "        'BPS_NAME' : 'BPS', \n",
    "        'FDST_TYPE' : 'LF22_FDST',\n",
    "        'FDST_SEV' : 'LF22_FDST', \n",
    "        'FDST_TSD' : 'LF22_FDST'\n",
    "    }\n",
    "\n",
    "    for feature in feature_list:\n",
    "        df[feature] = df[source_layers[feature]].map(LF_ref_dicts[feature]).copy()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Preprocess window dataframe__\n",
    "Prepares the data to be run through the model. Separates out null, non-disturbed, and agricultural/developed FVTs. Returns a dictionary with: \n",
    "1. A clean dataframe to be run through the model. \n",
    "2. A dataframe of the dropped observations to be rejoined to model predictions. This allows for the data to be reshaped to a 2D numpy array and written as a raster. \n",
    "\n",
    "Note: _Agricultural and developed FVTs are updated using additional datasets (e.g. the Cropland Data Layer for agricultural) and therefore aren't predicted using the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df):\n",
    "    \"\"\"\n",
    "    Prepares data to run model on.\n",
    "    Input: \n",
    "    - A dataframe containing created using the flattened numpy arrays returned from reading in the raster chunks. \n",
    "    Returns: \n",
    "    - Clean dataframe without NULL data or non-burnable F40 Classes \n",
    "    - Dataframe with the dropped observations - to be reappended after prediction\n",
    "    \"\"\"\n",
    "    # Join in additional features\n",
    "    df = join_features(df, feature_list = ['BPS_NAME'])\n",
    "\n",
    "    # Remove -9999/-1111 (Null values)\n",
    "    null_points = df[(df.isin([-1111, -9999])).any(axis=1)]  # Find rows with -1111/-9999 in any column\n",
    "    df = df.drop(null_points.index, axis=0)  # Drop those rows\n",
    "    \n",
    "    # Remove non-disturbed points\n",
    "    non_disturbed = df.loc[df['LF22_FDST'] == 0]\n",
    "    df = df.drop(non_disturbed.index, axis=0)\n",
    "\n",
    "    # Filter out agricultural and developed points\n",
    "    developed_fvt = list(range(20,33)) + list(range(2901,2906))\n",
    "    ag_fvt = [80, 81, 82] + list(range(2960, 2971))\n",
    "    fvt_filter = developed_fvt + ag_fvt\n",
    "\n",
    "    ag_dev = df.loc[df['LF20_FVT'].isin(fvt_filter)]\n",
    "    df = df.drop(ag_dev.index, axis=0)\n",
    "\n",
    "    # Join the filtered values together so they can be readded later\n",
    "    dropped = pd.concat([null_points, non_disturbed, ag_dev], axis=0)\n",
    "    dropped = dropped[['LF20_FVT']].rename(columns={'LF20_FVT':'FVT_Pred'})  # Renamed to match with predictions\n",
    "    \n",
    "    return df, dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Predict FVT__\n",
    "### Read Window\n",
    "This function is used to read in chunks of the rull raster to be processed. The raster data is stored as blocks with height=1 and width=41854 (the raster width), so we read in chunks composed of these blocks (e.g. 1000 blocks at a time). This corresponds to the row_slice argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_read(ras, row_slice):\n",
    "  \"\"\"\n",
    "  Reads in a subset (window) of the data to be processed.\n",
    "  Inputs:\n",
    "  - ras: Raster object read in using rasterio\n",
    "  - row_slice: Used to define the window height - in the form (row_start, row_end). \n",
    "\n",
    "  Returns:\n",
    "  - data: The data in the window as a 2D numpy array\n",
    "  - win: The Window object used to define the subset of the data. \n",
    "  - win_transform: The affine transform associated with the window. Used to update the metadata of the output of that chunk. \n",
    "  \"\"\"\n",
    "  \n",
    "  with rasterio.open(ras) as src:\n",
    "    col_slice = (0, src.width)  # Define row slice based on block size\n",
    "    win = Window.from_slices(row_slice, col_slice) \n",
    "    data = src.read(window=win)\n",
    "    win_transform = src.window_transform(win)\n",
    "    \n",
    "  return data, win, win_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Apply Trained Model__ \n",
    "Applies a trained model to the prepared data to predict FVT for each observation (i.e. pixel). After making predictions, rejoins the previously dropped observations so that the dataframe can be reshaped to a 2D numpy array and written to a raster. The index is used to keep track of relative pixel locations in the dataframe. This function returns a dataframe with predictions for each non-null and burnable observation. \n",
    "\n",
    "The model was not trained using null (-9999, -1111), non-disturbed, or agricultural/developed classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_FVT(model, df):\n",
    "    \"\"\"\n",
    "    Predicts F40 class given a trained model and data to predict on.\n",
    "    Returns:\n",
    "    - A dataframe of predicted F40 values joined to the previously dropped values. \n",
    "    \"\"\"\n",
    "    # Prep the data - get the a clean dataframe (i.e. no NULL data/nonburnable classes) and dropped observations\n",
    "    clean_df, dropped = data_prep(df)\n",
    "\n",
    "    # Get list of predictors for run\n",
    "    predictors = clean_df.columns.tolist()\n",
    "    \n",
    "    x_test = clean_df[predictors].copy()\n",
    "\n",
    "    # Run model to predict\n",
    "    # If clean_df is empty, then all values were NULL and are in dropped\n",
    "    if clean_df.shape[0] > 0:\n",
    "        y_pred = model.predict(x_test)\n",
    "    else:\n",
    "        return dropped\n",
    "    \n",
    "    # Join the dropped observations back in \n",
    "    # This allows the result dataframe to be reshaped back to a raster\n",
    "    df = pd.DataFrame({\"FVT_Pred\" : y_pred},\n",
    "                       index=x_test.index)\n",
    "    df = pd.concat([dropped, df])\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Return predictions\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_predict(row, model_fpath, win_height, raster_dict, ras_shape, out_dir, out_meta):\n",
    "\n",
    "    model = load(model_fpath)\n",
    "\n",
    "    row_start = row\n",
    "    row_end = row + win_height  # This is also the row_offset of the window\n",
    "\n",
    "    # make sure slice doesn't exceed row/col dims\n",
    "    if row_end > ras_shape[0]:\n",
    "        row_end = ras_shape[0]\n",
    "\n",
    "    # Define the window to be processed\n",
    "    row_slice = (row_start, row_end)\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    # For the current window, load data from each rasters\n",
    "    for var, fpath in raster_dict.items():\n",
    "        data_chunk, data_win, data_transform = windowed_read(fpath, row_slice)\n",
    "        data_dict[var] = data_chunk.ravel()\n",
    "\n",
    "    # Create a dataframe from the dictionary of datachunks\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # Look at the window currently processing\n",
    "    clear_output()\n",
    "\n",
    "    datetime = dt.datetime.now().strftime('%H-%M-%S-%f')\n",
    "    out_file = f\"data_chunk_{datetime}.tif\"\n",
    "    #print(f\"Row Slice: {row_slice}\")  \n",
    "    #print(f\"Writing {out_file}.\")\n",
    "    #print(f\"Processing window {i} of {floor(ras_shape[0] / win_height)}\")\n",
    "\n",
    "    # Run model to predict F40 Classes for window \n",
    "    out_arr = predict_FVT(model, df)\n",
    "\n",
    "    # Reshape to 2D\n",
    "    out_arr_np = out_arr.to_numpy()\n",
    "    out_arr_2D = out_arr_np.reshape(data_chunk.shape)\n",
    "    out_arr_2D = out_arr_2D[0]\n",
    "\n",
    "    # update output metadata for chunk\n",
    "    out_meta.update({\n",
    "    'height': out_arr_2D.shape[0],\n",
    "    'width': out_arr_2D.shape[1],\n",
    "    'transform' : data_transform\n",
    "    })\n",
    "\n",
    "    # Write chunk out\n",
    "    datetime = dt.datetime.now().strftime('%H-%M-%S-%f')\n",
    "    out_file = f\"data_chunk_{datetime}.tif\"\n",
    "    with rasterio.open(os.path.join(out_dir, out_file), 'w+', **out_meta) as out:\n",
    "        out.write(out_arr_2D, indexes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Run Model Predictions__\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mcco573\\AppData\\Local\\Temp\\ipykernel_17112\\1716466617.py\", line 23, in window_predict\n  File \"c:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\pandas\\core\\frame.py\", line 767, in __init__\n    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 503, in dict_to_mgr\n    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 114, in arrays_to_mgr\n    index = _extract_index(arrays)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 677, in _extract_index\n    raise ValueError(\"All arrays must be of the same length\")\nValueError: All arrays must be of the same length\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m win_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Number of rows to process at once\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Run predict function in parallel\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_fpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraster_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_meta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_height\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\geospatial\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Define paths to either pnnl or local root directory. \n",
    "\n",
    "# Define filepath to model\n",
    "model_fpath = os.path.join(paths_dict['model_dir'],  paths_dict['model_fname'])\n",
    "\n",
    "# Define the source raster file names\n",
    "raster_paths = {\n",
    "    \"LF20_FVT\" : os.path.join(paths_dict['data_dir'], \"LC22_FVT_220_bpa.tif\"),\n",
    "    \"LF22_FDST\" : os.path.join(paths_dict['data_dir'], \"LC22_FDst_230_bpa.tif\"),\n",
    "    \"LF20_FVC\" : os.path.join(paths_dict['data_dir'], \"LC22_FVC_220_bpa.tif\"),\n",
    "    \"LF20_FVH\" : os.path.join(paths_dict['data_dir'], \"LC22_FVH_220_bpa.tif\"),\n",
    "    \"ZONE\" : os.path.join(paths_dict['data_dir'], \"us_lf_zones_bpa.tif\"),\n",
    "    \"ASPECT\" : os.path.join(paths_dict['data_dir'], \"LC20_Asp_220_bpa.tif\"),\n",
    "    \"SLOPE\" : os.path.join(paths_dict['data_dir'], \"LC20_SlpD_220_bpa.tif\"),\n",
    "    \"ELEVATION\" : os.path.join(paths_dict['data_dir'], \"LC20_Elev_220_bpa.tif\"),\n",
    "    \"BPS\" : os.path.join(paths_dict['data_dir'], \"LC22_BPS_220_bpa.tif\")\n",
    "}\n",
    "\n",
    "# Create directory using current datetime to output data chunks to\n",
    "datetime = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # Used to name output file\n",
    "out_dir = make_dir(base_dir=paths_dict['out_base_dir'], new_dir_name=paths_dict['new_dir_name'])\n",
    "\n",
    "# Arbitrarily grab metadata from F40 raster to use for updating output metdata\n",
    "with rasterio.open(raster_paths['LF20_FVT']) as src:\n",
    "  out_meta = src.meta.copy()\n",
    "\n",
    "## Read in data\n",
    "# Open a raster to access its attributes\n",
    "ras = rasterio.open(raster_paths['LF20_FVT'])\n",
    "\n",
    "# Define window height and iteration tracker\n",
    "win_height = 1000  # Number of rows to process at once\n",
    "\n",
    "# Run predict function in parallel\n",
    "Parallel(n_jobs=24)(delayed(window_predict)(row, model_fpath, win_height, raster_paths, ras.shape, out_dir, out_meta) for row in range(0, ras.shape[0], win_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mosaic the Data Chunks\n",
    "Combines the chunks generated above into a single raster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file paths of the generated data chunks\n",
    "raster_fpaths = glob.glob(out_dir + \"/*.tif\")\n",
    "\n",
    "# Get the rasterio dataset objects corresponding to each path\n",
    "src_files_to_mosaic = []\n",
    "for fpath in raster_fpaths:\n",
    "    src = rasterio.open(fpath)\n",
    "    src_files_to_mosaic.append(src)\n",
    "\n",
    "# Merge the data chunks into a single raster\n",
    "mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "\n",
    "# Get the metadata for writing\n",
    "out_meta = src.meta.copy()\n",
    "out_meta.update({\n",
    "    \"driver\" : \"GTiff\",\n",
    "    \"height\" : mosaic.shape[1],\n",
    "    \"width\" : mosaic.shape[2],\n",
    "    \"transform\" : out_trans\n",
    "})\n",
    "\n",
    "# Write out the mosaic raster\n",
    "fname = out_dir + '.tif'\n",
    "with rasterio.open(os.path.join(out_dir, fname), \"w\", **out_meta) as dest:\n",
    "    dest.write(mosaic)\n",
    "\n",
    "print(f\"Raster written to {os.path.join(out_dir, fname)}\")\n",
    "\n",
    "# Delete the chunks\n",
    "#for raster in raster_fpaths:\n",
    "    #os.remove(raster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
